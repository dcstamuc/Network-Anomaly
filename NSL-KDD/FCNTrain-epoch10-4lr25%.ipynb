{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train__2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11850, 124)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test__2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25192, 122)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 3\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd-/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    \n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd-/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        print(\"********************************** Training ******************************\")\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [30]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-4]\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_allu9-.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allu9-.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "        past_scores.to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allu9-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:30\n",
      "Step 1 | Training Loss: 0.432325 | Validation Accuracy: 0.921825\n",
      "Accuracy on Test data: 0.8650195002555847, 0.7501266002655029\n",
      "Step 2 | Training Loss: 0.355137 | Validation Accuracy: 0.958730\n",
      "Accuracy on Test data: 0.8148953318595886, 0.6508016586303711\n",
      "Step 3 | Training Loss: 0.381115 | Validation Accuracy: 0.962302\n",
      "Accuracy on Test data: 0.7953335642814636, 0.6125738620758057\n",
      "Step 4 | Training Loss: 0.360334 | Validation Accuracy: 0.965873\n",
      "Accuracy on Test data: 0.7817600965499878, 0.5861603617668152\n",
      "Step 5 | Training Loss: 0.353615 | Validation Accuracy: 0.960317\n",
      "Accuracy on Test data: 0.7785663604736328, 0.5799155831336975\n",
      "Step 6 | Training Loss: 0.361443 | Validation Accuracy: 0.969841\n",
      "Accuracy on Test data: 0.7770581841468811, 0.5770463943481445\n",
      "Step 7 | Training Loss: 0.346162 | Validation Accuracy: 0.966270\n",
      "Accuracy on Test data: 0.76987224817276, 0.5633755326271057\n",
      "Step 8 | Training Loss: 0.347109 | Validation Accuracy: 0.973413\n",
      "Accuracy on Test data: 0.7524840235710144, 0.5302953720092773\n",
      "Step 9 | Training Loss: 0.353575 | Validation Accuracy: 0.972222\n",
      "Accuracy on Test data: 0.743923008441925, 0.5140084624290466\n",
      "Step 10 | Training Loss: 0.353564 | Validation Accuracy: 0.973810\n",
      "Accuracy on Test data: 0.7431688904762268, 0.5125738382339478\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:30\n",
      "Step 1 | Training Loss: 0.652529 | Validation Accuracy: 0.919444\n",
      "Accuracy on Test data: 0.8101490139961243, 0.646497905254364\n",
      "Step 2 | Training Loss: 0.520701 | Validation Accuracy: 0.956746\n",
      "Accuracy on Test data: 0.8353441953659058, 0.6900421977043152\n",
      "Step 3 | Training Loss: 0.431474 | Validation Accuracy: 0.958333\n",
      "Accuracy on Test data: 0.8329489231109619, 0.6849789023399353\n",
      "Step 4 | Training Loss: 0.424281 | Validation Accuracy: 0.957540\n",
      "Accuracy on Test data: 0.828335702419281, 0.6761181354522705\n",
      "Step 5 | Training Loss: 0.408214 | Validation Accuracy: 0.960714\n",
      "Accuracy on Test data: 0.7945795059204102, 0.6118143200874329\n",
      "Step 6 | Training Loss: 0.381388 | Validation Accuracy: 0.967857\n",
      "Accuracy on Test data: 0.7802963256835938, 0.5833755135536194\n",
      "Step 7 | Training Loss: 0.362159 | Validation Accuracy: 0.968254\n",
      "Accuracy on Test data: 0.7798084020614624, 0.5823628902435303\n",
      "Step 8 | Training Loss: 0.352703 | Validation Accuracy: 0.965873\n",
      "Accuracy on Test data: 0.7711586356163025, 0.5658227801322937\n",
      "Step 9 | Training Loss: 0.357420 | Validation Accuracy: 0.969444\n",
      "Accuracy on Test data: 0.7736426591873169, 0.5705485343933105\n",
      "Step 10 | Training Loss: 0.351434 | Validation Accuracy: 0.973413\n",
      "Accuracy on Test data: 0.7723119258880615, 0.5680168867111206\n"
     ]
    }
   ],
   "source": [
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsu9-.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsu9-__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_scoresu9-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allu9-.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>0.865020</td>\n",
       "      <td>0.869785</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.826120</td>\n",
       "      <td>1.066898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.835344</td>\n",
       "      <td>0.842552</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.787282</td>\n",
       "      <td>3.458654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.810149</td>\n",
       "      <td>0.815087</td>\n",
       "      <td>0.646498</td>\n",
       "      <td>0.751232</td>\n",
       "      <td>1.749366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "0      2              30              1     0.921825    0.865020  0.869785   \n",
       "2      3              30              3     0.956746    0.835344  0.842552   \n",
       "1      2              30              3     0.919444    0.810149  0.815087   \n",
       "\n",
       "   test_score_20  f1_score_20  time_taken  \n",
       "0       0.750127     0.826120    1.066898  \n",
       "2       0.690042     0.787282    3.458654  \n",
       "1       0.646498     0.751232    1.749366  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>0.865020</td>\n",
       "      <td>0.869785</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.826120</td>\n",
       "      <td>1.066898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>0.835344</td>\n",
       "      <td>0.842552</td>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.787282</td>\n",
       "      <td>3.458654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "30             1                  2     0.921825    0.865020  0.869785   \n",
       "               3                  3     0.956746    0.835344  0.842552   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "30             1                   0.750127     0.826120    1.066898  \n",
       "               3                   0.690042     0.787282    3.458654  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.921825</td>\n",
       "      <td>0.865020</td>\n",
       "      <td>0.869785</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.826120</td>\n",
       "      <td>1.066898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.822747</td>\n",
       "      <td>0.828819</td>\n",
       "      <td>0.668270</td>\n",
       "      <td>0.769257</td>\n",
       "      <td>2.604010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "30             1                2.0     0.921825    0.865020  0.869785   \n",
       "               3                2.5     0.938095    0.822747  0.828819   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "30             1                   0.750127     0.826120    1.066898  \n",
       "               3                   0.668270     0.769257    2.604010  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsu9-.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsu9-__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.panel.Panel'>\n",
       "Dimensions: 3 (items) x 22544 (major_axis) x 4 (minor_axis)\n",
       "Items axis: 2_30_1 to 3_30_3\n",
       "Major_axis axis: 0 to 22543\n",
       "Minor_axis axis: Actual to Prediction"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.115896</td>\n",
       "      <td>0.884104</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130775</td>\n",
       "      <td>0.869225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812566</td>\n",
       "      <td>0.187434</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109019</td>\n",
       "      <td>0.890982</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044361</td>\n",
       "      <td>0.955639</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958618</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698678</td>\n",
       "      <td>0.301322</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.991664</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953797</td>\n",
       "      <td>0.046203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.035677</td>\n",
       "      <td>0.964323</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.383176</td>\n",
       "      <td>0.616824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916326</td>\n",
       "      <td>0.083674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.863808</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117862</td>\n",
       "      <td>0.882138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.734585</td>\n",
       "      <td>0.265415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954609</td>\n",
       "      <td>0.045391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958550</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246288</td>\n",
       "      <td>0.753712</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.119851</td>\n",
       "      <td>0.880149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>0.983813</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013372</td>\n",
       "      <td>0.986628</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955780</td>\n",
       "      <td>0.044220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960396</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025312</td>\n",
       "      <td>0.974688</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.991770</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955203</td>\n",
       "      <td>0.044797</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957149</td>\n",
       "      <td>0.042851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>0.941267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242382</td>\n",
       "      <td>0.757618</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937185</td>\n",
       "      <td>0.062815</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959913</td>\n",
       "      <td>0.040087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378689</td>\n",
       "      <td>0.621311</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045289</td>\n",
       "      <td>0.954711</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.997187</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957519</td>\n",
       "      <td>0.042481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355562</td>\n",
       "      <td>0.644438</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.910713</td>\n",
       "      <td>0.089287</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787466</td>\n",
       "      <td>0.212534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.089974</td>\n",
       "      <td>0.910026</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961647</td>\n",
       "      <td>0.038353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.949633</td>\n",
       "      <td>0.050367</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.973930</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954964</td>\n",
       "      <td>0.045036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040655</td>\n",
       "      <td>0.959345</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945818</td>\n",
       "      <td>0.054182</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.094561</td>\n",
       "      <td>0.905439</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.676371</td>\n",
       "      <td>0.323629</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.180609</td>\n",
       "      <td>0.819391</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961109</td>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.939398</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116530</td>\n",
       "      <td>0.883470</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137921</td>\n",
       "      <td>0.862079</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952580</td>\n",
       "      <td>0.047420</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117337</td>\n",
       "      <td>0.882663</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104150</td>\n",
       "      <td>0.895850</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.598552</td>\n",
       "      <td>0.401448</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065419</td>\n",
       "      <td>0.934581</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.949997</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.340455</td>\n",
       "      <td>0.659545</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893548</td>\n",
       "      <td>0.106452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.905286</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935297</td>\n",
       "      <td>0.064704</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178945</td>\n",
       "      <td>0.821055</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.205198</td>\n",
       "      <td>0.794802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163821</td>\n",
       "      <td>0.836179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.992094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649286</td>\n",
       "      <td>0.350714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458177</td>\n",
       "      <td>0.541823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954757</td>\n",
       "      <td>0.045243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528092</td>\n",
       "      <td>0.471908</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.951631</td>\n",
       "      <td>0.048369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.122019</td>\n",
       "      <td>0.877981</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.176788</td>\n",
       "      <td>0.823212</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.406218</td>\n",
       "      <td>0.593782</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948403</td>\n",
       "      <td>0.051597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.956628</td>\n",
       "      <td>0.043372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22524</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680114</td>\n",
       "      <td>0.319886</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22525</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608855</td>\n",
       "      <td>0.391145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961188</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22527</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959927</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>0.748308</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.671215</td>\n",
       "      <td>0.328785</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.144432</td>\n",
       "      <td>0.855568</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081237</td>\n",
       "      <td>0.918763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958986</td>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959718</td>\n",
       "      <td>0.040282</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>0.886108</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960973</td>\n",
       "      <td>0.039027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22536</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920169</td>\n",
       "      <td>0.079831</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238695</td>\n",
       "      <td>0.761305</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946174</td>\n",
       "      <td>0.053826</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957153</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.948153</td>\n",
       "      <td>0.051847</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916247</td>\n",
       "      <td>0.083753</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22543</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242198</td>\n",
       "      <td>0.757802</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22544 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.115896     0.884104         1.0\n",
       "1         1.0     0.130775     0.869225         1.0\n",
       "2         0.0     0.812566     0.187434         0.0\n",
       "3         1.0     0.109019     0.890982         1.0\n",
       "4         1.0     0.044361     0.955639         1.0\n",
       "5         0.0     0.958618     0.041382         0.0\n",
       "6         0.0     0.698678     0.301322         0.0\n",
       "7         1.0     0.008336     0.991664         1.0\n",
       "8         0.0     0.953797     0.046203         0.0\n",
       "9         1.0     0.035677     0.964323         1.0\n",
       "10        1.0     0.383176     0.616824         1.0\n",
       "11        0.0     0.916326     0.083674         0.0\n",
       "12        1.0     0.136192     0.863808         1.0\n",
       "13        1.0     0.117862     0.882138         1.0\n",
       "14        0.0     0.734585     0.265415         0.0\n",
       "15        0.0     0.959500     0.040500         0.0\n",
       "16        0.0     0.954609     0.045391         0.0\n",
       "17        0.0     0.958550     0.041450         0.0\n",
       "18        0.0     0.246288     0.753712         1.0\n",
       "19        1.0     0.119851     0.880149         1.0\n",
       "20        1.0     0.016187     0.983813         1.0\n",
       "21        1.0     0.013372     0.986628         1.0\n",
       "22        0.0     0.955780     0.044220         0.0\n",
       "23        0.0     0.960396     0.039604         0.0\n",
       "24        1.0     0.025312     0.974688         1.0\n",
       "25        1.0     0.008230     0.991770         1.0\n",
       "26        0.0     0.955203     0.044797         0.0\n",
       "27        0.0     0.957149     0.042851         0.0\n",
       "28        1.0     0.058733     0.941267         1.0\n",
       "29        0.0     0.242382     0.757618         1.0\n",
       "30        1.0     0.604100     0.395900         0.0\n",
       "31        0.0     0.937185     0.062815         0.0\n",
       "32        0.0     0.959913     0.040087         0.0\n",
       "33        0.0     0.378689     0.621311         1.0\n",
       "34        1.0     0.045289     0.954711         1.0\n",
       "35        1.0     0.002813     0.997187         1.0\n",
       "36        0.0     0.957519     0.042481         0.0\n",
       "37        1.0     0.355562     0.644438         1.0\n",
       "38        0.0     0.910713     0.089287         0.0\n",
       "39        0.0     0.787466     0.212534         0.0\n",
       "40        1.0     0.089974     0.910026         1.0\n",
       "41        0.0     0.961647     0.038353         0.0\n",
       "42        0.0     0.955100     0.044900         0.0\n",
       "43        0.0     0.949633     0.050367         0.0\n",
       "44        1.0     0.026070     0.973930         1.0\n",
       "45        0.0     0.954964     0.045036         0.0\n",
       "46        1.0     0.040655     0.959345         1.0\n",
       "47        1.0     0.945818     0.054182         0.0\n",
       "48        1.0     0.094561     0.905439         1.0\n",
       "49        0.0     0.676371     0.323629         0.0\n",
       "...       ...          ...          ...         ...\n",
       "22494     1.0     0.180609     0.819391         1.0\n",
       "22495     0.0     0.961109     0.038891         0.0\n",
       "22496     1.0     0.060602     0.939398         1.0\n",
       "22497     1.0     0.116530     0.883470         1.0\n",
       "22498     1.0     0.137921     0.862079         1.0\n",
       "22499     0.0     0.952580     0.047420         0.0\n",
       "22500     1.0     0.117337     0.882663         1.0\n",
       "22501     1.0     0.104150     0.895850         1.0\n",
       "22502     1.0     0.598552     0.401448         0.0\n",
       "22503     1.0     0.065419     0.934581         1.0\n",
       "22504     1.0     0.050003     0.949997         1.0\n",
       "22505     1.0     0.340455     0.659545         1.0\n",
       "22506     0.0     0.893548     0.106452         0.0\n",
       "22507     0.0     0.905286     0.094714         0.0\n",
       "22508     0.0     0.935297     0.064704         0.0\n",
       "22509     1.0     0.178945     0.821055         1.0\n",
       "22510     1.0     0.205198     0.794802         1.0\n",
       "22511     0.0     0.953216     0.046784         0.0\n",
       "22512     1.0     0.163821     0.836179         1.0\n",
       "22513     1.0     0.007906     0.992094         1.0\n",
       "22514     0.0     0.649286     0.350714         0.0\n",
       "22515     1.0     0.458177     0.541823         1.0\n",
       "22516     0.0     0.954757     0.045243         0.0\n",
       "22517     1.0     0.528092     0.471908         0.0\n",
       "22518     0.0     0.951631     0.048369         0.0\n",
       "22519     1.0     0.122019     0.877981         1.0\n",
       "22520     1.0     0.176788     0.823212         1.0\n",
       "22521     1.0     0.406218     0.593782         1.0\n",
       "22522     1.0     0.948403     0.051597         0.0\n",
       "22523     0.0     0.956628     0.043372         0.0\n",
       "22524     1.0     0.680114     0.319886         0.0\n",
       "22525     1.0     0.608855     0.391145         0.0\n",
       "22526     0.0     0.961188     0.038812         0.0\n",
       "22527     0.0     0.959927     0.040073         0.0\n",
       "22528     1.0     0.251691     0.748308         1.0\n",
       "22529     0.0     0.671215     0.328785         0.0\n",
       "22530     1.0     0.144432     0.855568         1.0\n",
       "22531     1.0     0.081237     0.918763         1.0\n",
       "22532     0.0     0.958986     0.041014         0.0\n",
       "22533     0.0     0.959718     0.040282         0.0\n",
       "22534     1.0     0.113892     0.886108         1.0\n",
       "22535     0.0     0.960973     0.039027         0.0\n",
       "22536     1.0     0.021381     0.978619         1.0\n",
       "22537     1.0     0.920169     0.079831         0.0\n",
       "22538     1.0     0.238695     0.761305         1.0\n",
       "22539     0.0     0.946174     0.053826         0.0\n",
       "22540     0.0     0.957153     0.042847         0.0\n",
       "22541     1.0     0.948153     0.051847         0.0\n",
       "22542     0.0     0.916247     0.083753         0.0\n",
       "22543     1.0     0.242198     0.757802         1.0\n",
       "\n",
       "[22544 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions[\"3_30_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802599</td>\n",
       "      <td>0.197401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.676634</td>\n",
       "      <td>0.323366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116275</td>\n",
       "      <td>0.883725</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242791</td>\n",
       "      <td>0.757209</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>0.986824</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953387</td>\n",
       "      <td>0.046613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>0.985677</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.281408</td>\n",
       "      <td>0.718592</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454238</td>\n",
       "      <td>0.545762</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769240</td>\n",
       "      <td>0.230760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058681</td>\n",
       "      <td>0.941319</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058560</td>\n",
       "      <td>0.941440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203904</td>\n",
       "      <td>0.796096</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707606</td>\n",
       "      <td>0.292394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.178990</td>\n",
       "      <td>0.821010</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>0.986211</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.418949</td>\n",
       "      <td>0.581051</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.976896</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190118</td>\n",
       "      <td>0.809882</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378036</td>\n",
       "      <td>0.621964</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193552</td>\n",
       "      <td>0.806448</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.818997</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.675081</td>\n",
       "      <td>0.324919</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.116851</td>\n",
       "      <td>0.883149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.448543</td>\n",
       "      <td>0.551457</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707320</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947018</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.755298</td>\n",
       "      <td>0.244702</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.975304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081608</td>\n",
       "      <td>0.918392</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.136342</td>\n",
       "      <td>0.863658</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899616</td>\n",
       "      <td>0.100384</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736184</td>\n",
       "      <td>0.263816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.128757</td>\n",
       "      <td>0.871243</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751699</td>\n",
       "      <td>0.248301</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074755</td>\n",
       "      <td>0.925245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437604</td>\n",
       "      <td>0.562396</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031796</td>\n",
       "      <td>0.968204</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085373</td>\n",
       "      <td>0.914627</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948926</td>\n",
       "      <td>0.051074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947588</td>\n",
       "      <td>0.052412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.120613</td>\n",
       "      <td>0.879387</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833810</td>\n",
       "      <td>0.166190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.969249</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015832</td>\n",
       "      <td>0.984168</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.065898</td>\n",
       "      <td>0.934102</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.708473</td>\n",
       "      <td>0.291527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>0.973077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>0.943381</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11800</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833408</td>\n",
       "      <td>0.166592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235969</td>\n",
       "      <td>0.764031</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.551265</td>\n",
       "      <td>0.448735</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.118503</td>\n",
       "      <td>0.881497</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104987</td>\n",
       "      <td>0.895013</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11805</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.070765</td>\n",
       "      <td>0.929235</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902230</td>\n",
       "      <td>0.097770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.126020</td>\n",
       "      <td>0.873980</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007649</td>\n",
       "      <td>0.992351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052912</td>\n",
       "      <td>0.947088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079390</td>\n",
       "      <td>0.920610</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.384942</td>\n",
       "      <td>0.615058</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612526</td>\n",
       "      <td>0.387474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11813</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021376</td>\n",
       "      <td>0.978624</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11815</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325552</td>\n",
       "      <td>0.674448</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.115949</td>\n",
       "      <td>0.884051</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893237</td>\n",
       "      <td>0.106763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.901154</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>0.984429</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.708565</td>\n",
       "      <td>0.291435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.992114</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>0.982047</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446699</td>\n",
       "      <td>0.553301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843283</td>\n",
       "      <td>0.156716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.939646</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.981347</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.990230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.149102</td>\n",
       "      <td>0.850898</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079496</td>\n",
       "      <td>0.920504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.389937</td>\n",
       "      <td>0.610063</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106585</td>\n",
       "      <td>0.893415</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027145</td>\n",
       "      <td>0.972855</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778433</td>\n",
       "      <td>0.221567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.107530</td>\n",
       "      <td>0.892470</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902486</td>\n",
       "      <td>0.097514</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.978478</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.080493</td>\n",
       "      <td>0.919507</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.245861</td>\n",
       "      <td>0.754139</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675015</td>\n",
       "      <td>0.324985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575176</td>\n",
       "      <td>0.424824</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.073921</td>\n",
       "      <td>0.926078</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.133923</td>\n",
       "      <td>0.866077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897388</td>\n",
       "      <td>0.102612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.948072</td>\n",
       "      <td>0.051928</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.048597</td>\n",
       "      <td>0.951403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.275346</td>\n",
       "      <td>0.724654</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.708565</td>\n",
       "      <td>0.291435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11850 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.802599     0.197401         0.0\n",
       "1         1.0     0.676634     0.323366         0.0\n",
       "2         1.0     0.116275     0.883725         1.0\n",
       "3         0.0     0.242791     0.757209         1.0\n",
       "4         1.0     0.013176     0.986824         1.0\n",
       "5         1.0     0.953387     0.046613         0.0\n",
       "6         1.0     0.014323     0.985677         1.0\n",
       "7         1.0     0.281408     0.718592         1.0\n",
       "8         0.0     0.454238     0.545762         1.0\n",
       "9         0.0     0.769240     0.230760         0.0\n",
       "10        1.0     0.058681     0.941319         1.0\n",
       "11        1.0     0.058560     0.941440         1.0\n",
       "12        1.0     0.203904     0.796096         1.0\n",
       "13        1.0     0.707606     0.292394         0.0\n",
       "14        1.0     0.178990     0.821010         1.0\n",
       "15        1.0     0.013789     0.986211         1.0\n",
       "16        1.0     0.418949     0.581051         1.0\n",
       "17        1.0     0.023104     0.976896         1.0\n",
       "18        1.0     0.190118     0.809882         1.0\n",
       "19        1.0     0.378036     0.621964         1.0\n",
       "20        1.0     0.193552     0.806448         1.0\n",
       "21        1.0     0.181003     0.818997         1.0\n",
       "22        1.0     0.675081     0.324919         0.0\n",
       "23        1.0     0.116851     0.883149         1.0\n",
       "24        1.0     0.448543     0.551457         1.0\n",
       "25        1.0     0.707320     0.292680         0.0\n",
       "26        1.0     0.947018     0.052982         0.0\n",
       "27        1.0     0.755298     0.244702         0.0\n",
       "28        0.0     0.886600     0.113400         0.0\n",
       "29        1.0     0.024696     0.975304         1.0\n",
       "30        1.0     0.081608     0.918392         1.0\n",
       "31        1.0     0.136342     0.863658         1.0\n",
       "32        0.0     0.899616     0.100384         0.0\n",
       "33        0.0     0.736184     0.263816         0.0\n",
       "34        1.0     0.128757     0.871243         1.0\n",
       "35        0.0     0.751699     0.248301         0.0\n",
       "36        1.0     0.074755     0.925245         1.0\n",
       "37        1.0     0.437604     0.562396         1.0\n",
       "38        1.0     0.031796     0.968204         1.0\n",
       "39        1.0     0.085373     0.914627         1.0\n",
       "40        0.0     0.948926     0.051074         0.0\n",
       "41        1.0     0.947588     0.052412         0.0\n",
       "42        1.0     0.120613     0.879387         1.0\n",
       "43        0.0     0.833810     0.166190         0.0\n",
       "44        1.0     0.030751     0.969249         1.0\n",
       "45        1.0     0.015832     0.984168         1.0\n",
       "46        1.0     0.065898     0.934102         1.0\n",
       "47        1.0     0.708473     0.291527         0.0\n",
       "48        1.0     0.026923     0.973077         1.0\n",
       "49        1.0     0.056619     0.943381         1.0\n",
       "...       ...          ...          ...         ...\n",
       "11800     1.0     0.833408     0.166592         0.0\n",
       "11801     1.0     0.235969     0.764031         1.0\n",
       "11802     0.0     0.551265     0.448735         0.0\n",
       "11803     1.0     0.118503     0.881497         1.0\n",
       "11804     1.0     0.104987     0.895013         1.0\n",
       "11805     1.0     0.070765     0.929235         1.0\n",
       "11806     0.0     0.902230     0.097770         0.0\n",
       "11807     1.0     0.126020     0.873980         1.0\n",
       "11808     1.0     0.007649     0.992351         1.0\n",
       "11809     1.0     0.052912     0.947088         1.0\n",
       "11810     1.0     0.079390     0.920610         1.0\n",
       "11811     1.0     0.384942     0.615058         1.0\n",
       "11812     0.0     0.612526     0.387474         0.0\n",
       "11813     1.0     0.021376     0.978624         1.0\n",
       "11814     1.0     0.001141     0.998859         1.0\n",
       "11815     1.0     0.325552     0.674448         1.0\n",
       "11816     1.0     0.115949     0.884051         1.0\n",
       "11817     0.0     0.893237     0.106763         0.0\n",
       "11818     0.0     0.901154     0.098846         0.0\n",
       "11819     1.0     0.015571     0.984429         1.0\n",
       "11820     1.0     0.708565     0.291435         0.0\n",
       "11821     1.0     0.007886     0.992114         1.0\n",
       "11822     1.0     0.017953     0.982047         1.0\n",
       "11823     1.0     0.446699     0.553301         1.0\n",
       "11824     1.0     0.843283     0.156716         0.0\n",
       "11825     0.0     0.939646     0.060354         0.0\n",
       "11826     1.0     0.018653     0.981347         1.0\n",
       "11827     1.0     0.009770     0.990230         1.0\n",
       "11828     1.0     0.149102     0.850898         1.0\n",
       "11829     1.0     0.079496     0.920504         1.0\n",
       "11830     1.0     0.389937     0.610063         1.0\n",
       "11831     1.0     0.106585     0.893415         1.0\n",
       "11832     1.0     0.027145     0.972855         1.0\n",
       "11833     0.0     0.778433     0.221567         0.0\n",
       "11834     1.0     0.107530     0.892470         1.0\n",
       "11835     0.0     0.902486     0.097514         0.0\n",
       "11836     1.0     0.007882     0.992118         1.0\n",
       "11837     1.0     0.021522     0.978478         1.0\n",
       "11838     1.0     0.080493     0.919507         1.0\n",
       "11839     1.0     0.245861     0.754139         1.0\n",
       "11840     0.0     0.675015     0.324985         0.0\n",
       "11841     0.0     0.844377     0.155623         0.0\n",
       "11842     1.0     0.575176     0.424824         0.0\n",
       "11843     1.0     0.073921     0.926078         1.0\n",
       "11844     1.0     0.133923     0.866077         1.0\n",
       "11845     0.0     0.897388     0.102612         0.0\n",
       "11846     0.0     0.948072     0.051928         0.0\n",
       "11847     1.0     0.048597     0.951403         1.0\n",
       "11848     1.0     0.275346     0.724654         1.0\n",
       "11849     1.0     0.708565     0.291435         0.0\n",
       "\n",
       "[11850 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"3_30_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train.predictions[\"3_30_3\"].dropna()\n",
    "df_ = Train.predictions_[\"3_30_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835344</td>\n",
       "      <td>0.842552</td>\n",
       "      <td>0.924509</td>\n",
       "      <td>0.773942</td>\n",
       "      <td>Train-/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.690042</td>\n",
       "      <td>0.787282</td>\n",
       "      <td>0.898005</td>\n",
       "      <td>0.700866</td>\n",
       "      <td>Train-/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision    Recall      Scenario\n",
       "0  0.835344  0.842552   0.924509  0.773942  Train-/Test+\n",
       "1  0.690042  0.787282   0.898005  0.700866  Train-/Test-"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train-/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train-/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAGhCAYAAADr81oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XWcFdUbx/HPsywN0oiENAhKd6qAhFLSrYjdiIHYgWIhovxUsECQEAwEQREMuhFEUSmlpEOaXc7vjzu7LrC7LHA37tzv+/W6L3Zmzsw8gyvPfc6cOWPOOUREREJZRGoHICIicqGUzEREJOQpmYmISMhTMhMRkZCnZCYiIiFPyUxEREKekpmIiIQ8JTMREQl5SmYiIhLyIlM7ABERST7pLirqXNSRoBzLHdn5jXOueVAOFmRKZiIiPuaijpCxbKegHOvoimF5g3KgZKBkJiLiawbm/ztK/r9CERHxPVVmIiJ+ZoBZakeR7JTMRET8Tt2MIiIiaZ8qMxERv1M3o4iIhDaNZhQREQkJqsxERPxO3YwiIhLSDHUzioiIhAJVZiIivmbqZhQRER9QN6OIiEjap8pMRMTv1M0oIiKhTQ9Ni4iIhARVZiIifhYmr4BRZSYiIiFPlZmIiN+FwT0zJTMREV/TABAREZGQoMpMRMTvIvw/AETJTETEzzRrvoiISGhQZSYi4ndh8JyZkpmIiK9pNKOIiEhIUGUmIuJ36mYUEZGQp25GERGRtE+VmYiIn5mFRTejKjMREQl5qsxERPxO98xEQpuZZTazr8xsv5l9egHH6W5m3wYzttRiZg3M7PfUjkNSUExX44V+0jAlM0kTzKybmS0xs4Nmts3MpplZ/SAcugNwMZDHOdfxfA/inBvjnGsahHiSlZk5MyuVWBvn3GznXNmUikkkJaibUVKdmT0A9AduB74BjgPNgTbAnAs8fFHgD+dc1AUexxfMLFJ/F+FGM4CIJDszywE8C9zlnPvMOXfIOXfCOfeVc+4hr01GMxtiZlu9zxAzy+htu8rMNptZPzPb4VV1vb1tzwBPAp29iq+PmT1tZqPjnL+YV81Eess3mtl6M/vXzDaYWfc46+fE2a+umS32ui8Xm1ndONt+MLPnzGyud5xvzSxvAtcfE//DceJva2bXmtkfZrbHzAbEaV/TzOab2T6v7VtmlsHb9pPX7GfvejvHOf4jZvYP8GHMOm+fkt45qnrLBc1sl5lddUH/YSVtUTejSLKrA2QCPk+kzWNAbaAyUAmoCTweZ3sBIAdQCOgDDDOzXM65p4AXgPHOuWzOufcTC8TMsgJDgRbOuexAXWBFPO1yA1O9tnmAwcBUM8sTp1k3oDeQH8gAPJjIqQsQ+DsoRCD5jgB6ANWABsCTZlbCaxsN9AXyEvi7awzcCeCca+i1qeRd7/g4x89NoEq9Ne6JnXPrgEeAMWaWBfgQ+Mg590Mi8YqkOUpmktryALvO0vXVHXjWObfDObcTeAboGWf7CW/7Cefc18BB4HzvCZ0ErjCzzM65bc651fG0uQ740zn3sXMuyjk3FlgDtIrT5kPn3B/OuSPABAKJOCEngIHOuRPAOAKJ6g3n3L/e+VcDFQGcc0udcwu8824E3gWuTMI1PeWcO+bFcwrn3AjgT2AhcAmBLw/iFzHvMwvGJw1L29FJONgN5I3p5ktAQeCvOMt/eetij3FaMjwMZDvXQJxzh4DOBO7dbTOzqWZ2WRLiiYmpUJzlf84hnt3OuWjv55hksz3O9iMx+5tZGTObYmb/mNkBApVnvF2Ycex0zh09S5sRwBXAm865Y2dpKyHFlMxEUsB84CjQNpE2Wwl0kcW41Ft3Pg4BWeIsF4i70Tn3jXPuGgIVyhoC/8ifLZ6YmLacZ0zn4m0CcZV2zl0EDCDw3TsxLrGNZpYNGAK8DzztdaOKhBQlM0lVzrn9BO4TDfMGPmQxs/Rm1sLMXvaajQUeN7N83kCKJ4HRCR3zLFYADc3sUm/wyaMxG8zsYjNr7d07O0aguzI6nmN8DZTxHieINLPOQHlgynnGdC6yAweAg17VeMdp27cDJc7YK3FvAEudczcTuBf4zgVHKWmLBoCIJD/n3GDgAQKDOnYCm4C7gS+8Js8DS4CVwCpgmbfufM41AxjvHWsppyagCKAfgcprD4F7UXfGc4zdQEuv7W7gYaClc27X+cR0jh4kMLjkXwJV4/jTtj8NjPRGO3Y628HMrA2BxyBu91Y9AFSNGcUpPhEG3YzmXKI9ECIiEsIichZ1Ga8ccPaGSXB08u1LnXPVg3KwINND0yIifpfGuwiDQclMRMTPTDOAiIiIhARVZiIifqduRv+zDNmcZclz9oYiZ1GlZL7UDkF8Ytmypbucc/qFOgdKZlnykLFB/9QOQ3xg7uenP/Ilcn4yp7fTZ5i5IKbKTEREQpkRHslMA0BERCTkqTITEfEz4+yzd/qAkpmIiK+ZuhlFRERCgSozERGfC4fKTMlMRMTnwiGZqZtRRERCniozERGfC4fKTMlMRMTPwmRovroZRUQk5KkyExHxMQuT58yUzEREfC4ckpm6GUVEJOSpMhMR8TlVZiIiIiFAlZmIiM+FQ2WmZCYi4md6zkxERCQ0qDITEfE5dTOKiEhIC5eHptXNKCIiIU+VmYiIz4VDZaZkJiLid/7PZepmFBGR0KfKTETEz0zdjCIi4gPhkMzUzSgiIiFPyUxExOfMLCifJJ6rr5mtNrNfzGysmWUys+JmttDM/jSz8WaWwWub0Vte620vFuc4j3rrfzezZmc7r5KZiIiPxTw0nRLJzMwKAfcC1Z1zVwDpgC7AS8DrzrnSwF6gj7dLH2Cvc64U8LrXDjMr7+13OdAc+J+ZpUvs3EpmIiISTJFAZjOLBLIA24BGwERv+0igrfdzG28Zb3tjC2TNNsA459wx59wGYC1QM7GTKpmJiPidBekDec1sSZzPrXFP45zbArwK/E0gie0HlgL7nHNRXrPNQCHv50LAJm/fKK99nrjr49knXhrNKCIiSbXLOVc9oY1mlotAVVUc2Ad8CrSIp6mL2SWBbQmtT5CSmYiIn6Xsc2ZNgA3OuZ0AZvYZUBfIaWaRXvVVGNjqtd8MFAE2e92SOYA9cdbHiLtPvNTNKCLicyk4mvFvoLaZZfHufTUGfgW+Bzp4bW4AvvR+nuwt422f5Zxz3vou3mjH4kBpYFFiJ1ZlJiIiQeGcW2hmE4FlQBSwHBgOTAXGmdnz3rr3vV3eBz42s7UEKrIu3nFWm9kEAokwCrjLORed2LmVzEREfC4lZwBxzj0FPHXa6vXEMxrROXcU6JjAcQYCA5N6XiUzERG/8/9sVrpnJiIioU+VmYiIz4XDRMNKZiIiPnYu8yqGMnUzikhQDB3yOlUrXU61ylfQq0dXjh49CsDbw97i8stKkTm9sWvXrtj2v69Zw5X165Aja0ZeH/xqaoUtPqFkJiIXbMuWLfxv2FDmLljC0hW/EB0dzafjxwFQp249vp7+HZcWLXrKPrly5+a114dy/wMPpkbIYSUlZ81PLepmFJGgiIqK4siRI6RPn54jhw9zScGCAFSuUiXe9vnz5yd//vxMnzY1JcMMS2k9EQWDKjMRuWCFChXi/r4PUqbEpRQvcgkXXZSDJtc0Te2wJIwomYnIBdu7dy9TvvqS3/7cwPq/t3Lo8CHGjhmd2mFJjODNmp9mKZmJyAWbNfM7ihUrTr58+UifPj1t27Zjwfx5qR2WhBElMxG5YEWKXMqiRQs4fPgwzjm+nzWTspeVS+2wxBMOA0CUzETkgtWsVYvr23WgTs2qVK9SgZMnT9LnlsB7G4e9OZSSxQqzZfNmalStyB233gzAP//8Q8lihRk6ZDAvvfA8JYsV5sCBA6l5Gf5k4ZHMLDDbfviKyFnUZWzQP7XDEB/Y+/kdqR2C+ETm9LY0sZdgnouMBUq7wt2HBuNQrB98bdDiCjYNzRcR8TED0nhRFRRKZiIivpb2uwiDQffMREQk5CmZhaF72lRk6bDOLHmrMyMfbELG9OkAuLJiIeYN6cCStzoz4v5GpIv479vca7fW45d3u7FoaCcql8wbu757o7Ksercrq97tSvdGZVP8WuTclC1VjOqVK1CrWmVqVavM/HmJD5/PmzPbBZ/zlptu5LLSxalVrTJ1alRlwfz553yMKV9N5pWXBwEw+csv+O3XX2O3Pfv0k8ya+d0Fx+lnZsH5pGXqZgwzBXNn5c5WFahy5ziOHo9m9CPX0LFhKcbM+p337m9Ei8cns3brfp7oXoMejcsycsYamlW7lJIFc3LFbZ9Qs+zFDL2jIQ0f/Ixc2TLyWNfq1Os7Eecc84Z0ZOrCDew7dDy1L1MSMf2778mbN+/ZGwbRC4NeoV37Dnw341vuufM2Fi9feU77t2zVmpatWgPw1Zdf0OK6lpQrXx6AJ59+Nujx+o26GcWXIiMiyJwhknQRRuaMkWzbc4g82TNx7EQ0a7fuB2DW8s20rVsCgJa1i/HJrN8BWPT7dnJkzUiBXFm4pmoRZq7YxN6Dx9h36DgzV2yiabVLU+265PwcPHiQFk0bU6dGVapXrsBXk788o822bdtocnVDalWrTLXKVzBnzmwAvpvxLVfWr0OdGlXp1qUjBw8eTPRc9Rs0ZN26tQD8vGIFDevVpkaVinTqcD179+4FAkP5q1QsT40qFenZvQsAH4/8iPvvvZv58+YxdcpkBvR/iFrVKrN+3TpuuelGPps0kW+mT6N7106x5/rpxx9o37bVecUpoUfJLMxs3XOIIZ+v4I8PerJh1A0cOHScmcs3s+vAUdJHRlC1VD4Arq9XgsJ5A11MBfNkZfOu//7n37L7IAXzZA2s3xln/a5DFMyTNWUvSM5Z8yZXU6taZRrUrQVApkyZGD/xc+YvXsb0776n/8P9OP2RnfHjPuGaps1YuHQFi5b+TKVKldm1axeDXnier7/5jvmLl1G1WnWGDhmc6LmnTvmKy6+oAMDNvXsx8MWXWLx8JVdcUYGBzz0DwKuvDGLB4uUsXr6SN4e9c8r+derW5bqWrXlh0CssXLqCEiVLxm5r3OQaFi1cwKFDhwCYOGE8HTp2Pq84fSVIXYxpvbhTN2OYyZk1Ay1rFafczaPZd+g4n/RvSperSjPuhz/p9fIMXr65HhnTR/Dd8s1ERZ8EwOKZlM05l8D6ZL8EuUCndzM653jy8QHMnf0TERERbN2yhe3bt1OgQIHYNtWr1+C2W27ixIkTtGrdlkqVKzP7px9Z89uvNGpYD4DjJ45Tq1adeM85oP9DvPTC8+TNl493hr/P/v372bd/Hw0aXglAj5430L1LRwAqVKjIjb2607p1W1q1aZvk64qMjKRp0+ZMnfIV7dp3YNq0qQwc9PI5xelHBkREpPFMFARKZmGmUeXCbNx+gF0HAi9O/GLeemqXK8C4H/5k4e/badL/CwAaVylM6UI5gEAlFlOlARTKk41tew6zZfdBGlQo+N/6vFmZvWprCl6NBMO4T8awa9dO5i1aSvr06SlbqhjHvBdrxqjfoCEzZv3E9K+n0ufGnvTt9xA5c+WiUZNrGDV67FnPEXPPLMb+/fsTbPv55KnMmf0TU76azIsvPMeyn1cn+Vo6dOrMO28PI3fu3FSrXoPs2bPjnEtynBK61M0YZjbtPEjNyy4mc8bA95irKxXm902BexX5cmQGIENkBP3aV2HEtMCIsakLN9LNG6lYs+zFHDh8jH/2HmbGsk00qVKEnFkzkDNrBppUKcKMZZtS4arkQuzfv598+fKTPn16fvzhe/7+668z2vz111/kz5+fm26+hRt692H58mXUrFWb+fPmsm5t4B7Y4cOH+fOPP5J0zhw5cpArZ67Ye2+fjPmY+g2v5OTJk2zetIkrr7qaFwa9zP59+864v5Ute3YO/vtvvMdteOVVrFi+jA/eH0GHjp0BLihOv1A3o/jO4j928Pnc9cwf0oGoaMfP63fy/vRA0urbrjItahQlwowR01bz48otAExf8jfNqhdl9fBuHD4WxW1vfA/A3oPHeHHcUuYMDnzjfmHsEvYePJY6FybnrUu37rRv24p6tapTsVJlyl522RltZv/4A68PfoX0kenJmi0b7384inz58jHi/Y/o1aMrx48F/rs/9ezzlC5TJknnHfHBSO6563aOHD5MsRIlGP7eh0RHR9P7hh4c2L8fh+Pu+/qSM2fOU/br2KkLd91xC/97ayifjJ94yrZ06dLR4tqWjB71Ee99MBLgguP0g3AYzai5GTU3owSJ5maUYAnm3IyZLynjSvYZFoxDsXpgU83NKCIiqSAEugiDQffMREQk5KkyS+N+erUdGdKnI3f2jGTKEMnW3YFnaDoNnM7fO+K/CX4+SlxyEauHd+fe//3EiGmB0WND72zIvF+3Me6HP4N2nlzZMtK+fkne8+7TFc6blRdvqkvPl2cE7RySdA3q1uL4sWPs2buHo0eOULBgIQAmTPqCosWKBf18Tz/5OHny5OWe++6nd68eXN++A61PG37fu1cP5s+fS46LAqNps2XPzswfZgc9lnARmDXf/6WZklka1/DBzwDo0bgs1Urlo++7c+JtFxFhnDx5Yfc//9l7mHvaVOSDb34l+gKPlZBc2TNyc4vLY5PZ5l2HlMhS0ex5C4HADBtLly5hyNC3UjmigJdfff2MJBdXVFQUkZGRCS4ndb/woFnzJQ1LF2FsG3sTT/WoyezX2lGjdH7WftiTHFkzAIEh9FOfC0zlkzVTJMPvv5rZr7Vj/pAOXFujaLzH3L73MHNXb6Pb1WeO8ip5SQ4mP3Mdc1/vwIwX21CqYI7Y9T+92o7Zr7Xjye412Db2JgCyZ07PtOdbM29IBxYN7UQL75zP31CbMoVysuCNjjx3Qy1KXHIRC94IPCw7Z3B7Shf6b+TazJfaUqFYniTHL8Hz3vB3efSRh2KXh7/zNgP6P8y6tWupWulybrqhJ9UrV6B7104cOXIEgCWLF3NNoyupW7MabVq2YPv27UGN6eknH+fuO27juubXcGuf3nz4/nv07N6Fdm1a0qZlC06ePMnDDz5AtcpXUL1yBT6bFBjpOGvmd1zbrAk9u3ehdvUqQY1J0g4lsxCWM1tGVqzbSYN+n7Hw94T/4RjQpTozlm6iQb/PaPHYZAb1qRs7U/7pXpm4jL7tKp9xw3jY3Vdy39uzqdd3Ik+OWsjrtzcAYPBt9Rny+Qoa9PuMHfuOxLY/cjyajgOnUff+iVz3xFe8fHNdAB4fuYA/tuyj9n2f8sTIhaecY9LsdbSvH5ieqFCerOTOnolVG3efU/wSHJ27duPLLz4jKioKgFEjP6RnrxsB+O3XX7ntjrtYsmIVmTJm4r3h73Ls2DEefOA+xk6YxLxFS+nSrQfPPvXEeZ//4Qf7xs7s3+fGXrHrV6xYzqQvvuKDkR8DsHDBfN7/6GOmTp/BpImfsua3X1m09GemTJ/Bww/2ZceOHQAsWriAgS++zJIVq847plCm58wugJk5YLBzrp+3/CCQzTn3dHKdM54YPgKmOOcmnq1tKDp2Ipov5284a7vGVYrQtNql9OsQ+FaaKUM6iuTLFjupcFzrtx1g5YbddGxQKnZdjqwZqFn2YsY+2ix2XWS6wPegGmUvpu0zUwEY/+OfPNWjJhD4xX/+xtrULXcJJ52jcN5s5LkoU6JxTpqzlolPXMug8Uvp0KAUk+asO+f4JTiyZ89O/foN+Wb6NIoXL0G6dOkoV74869aupVjx4tSqXRuArt178P57w2l45VX89utqrmvWBIDo6GgKFS583udPqJuxVes2ZMr03+9RkyZNyZUrFwDz5s6hc5dupEuXjgIFClC3Xn2WLV1ChgwZqFW7DpdeGr6TYIdDN2Nydh4fA9qZ2YvOuV3nurOZRTrnopIhLt84cuzUv56o6JNEeL+0cSsXs8CAkQ3/HEjScV+asJSPHmzCIq/aM2D3gaPUvu/TJMfWvVFZcmTJQJ37PyX6pGPthz3JdJZq6u+dBzl09ASXFclFhwYluWXI9+cVvwTHjTfdzNA3BlO0aDF63dA7dv3p/zCaGc45rqhQMdkHamTJcupE1lmy/rec2DOzcduJPyVnN2MUMBzoe/oGMytqZjPNbKX356Xe+o/MbLCZfQ+8ZGZPm9lIM/vWzDaaWTsze9nMVpnZdDNL7+33pJktNrNfzGy4hcPXkHj8teNfqsTMeu+9vgXgu2WbuKtVhdjlSiUSf5fVb3/vZcO2AzTzXuey79Bx/tl7iNa1iwOB5FKhWB4AlvyxnTZ1Aufq2DBONZclAzv3HyH6pKNR5cIU8uZ2PHjkBNkzp0/w3BNnr+WhjlXIkD4da7xpts41fgmOuvXqsWHdOj6b9CkdOnWOXb9xwwaWLF4MwIRxY6lbtz7lypdn69YtLF60CIDjx4/z6+qkz6kYDPUbNOTTCeOIjo5m+/btzJ83l6rV0uTzvSkrTGbNT+57ZsOA7maW47T1bwGjnHMVgTHA0DjbygBNYrongZLAdUAbYDTwvXOuAnDEWw/wlnOuhnPuCiAz0DJZriaNe/6TJbxxRwO+G9SW41HRsesHjl1C5oyRLH6zE0uHdeaxrmf/H3zQhKUUyZc9drnnyzO4uUV5Fg7tyLJhXWhRMzAIo9/wufRrX5nZr7UjX47MHDgceDHnJ9//Qe3LCjBncHva1SvJn1v2AbBj3xGWrd3J4jc78dwNtc4472dz19G5YenYLsbzjV+C4/r2HahfvyE5cvz3v3D58pfz4fsjqFGlIocOH6LPLbeSMWNGPhk3kUceeoCaVStRu0YVFi9amMiRExf3nlmtapWJjo4+6z7t2negTNnLqFmtEtc1a8JLrwwmf/785x2DX8QMzQ/GJy1LtumszOygcy6bmT0LnCCQfLI55542s13AJc65E151tc05l9e7x/W9c26kd4yngRPOuYFmFuEdI5NzznnH3eOcG2Jm7YGHgSxAbuBN59yghO6ZmdmtwK0AZM5dLVPj55Pl7yAcZMkYyWGvu7PLVaVpU6cEXV/8JpWjSh1+nM6q9XXNeeiRR2Nf1bJu7Vq6de7AwqUrUjkyfwvmdFZZC5V1l93+ztkbJsGyJxuF9XRWQ4BlwIeJtImbUQ+dtu0YgHPupJmdcP9l35NApJllAv4HVHfObfISYKIjDZxzwwl0gRKRs2h4T055gaqVzs8rt9Qjwox9h45x65BZqR2SBMHu3bu5qkEdqlStFpvIJHSl8aIqKJI9mTnn9pjZBKAP8IG3eh7QBfgY6A7E/yRw0sQkrl1mlg3oAPhy9GJaNPuXrec0MERCQ548eVj165mvSSlZqpSqshCU1rsIgyGlnjN7DYh71/5eoLeZrQR6Aved74Gdc/uAEcAq4Atg8QXEKSIiISjZKjPnXLY4P28ncD8rZnkj0CiefW48bfnpRI75dJyfHwceP9vxRETCURgUZpqbUUTE10zdjCIiIiFBlZmIiI8FnjNL7SiSnyozEREJearMRER8Le3P3hEMSmYiIj4XBrlM3YwiIhL6VJmJiPicuhlFRCS0hcDrW4JB3YwiIhLyVJmJiPhYzPvM/E7JTETE58IhmambUUREQp4qMxERnwuDwkzJTETE79TNKCIiEgJUmYmI+JmeMxMREQkNqsxERHzMNGu+iIj4QRjkMnUziohI6FNlJiLicxFhUJopmYmI+FwY5DJ1M4qISOhTZSYi4mNm4TEDiJKZiIjPRfg/l6mbUUREQp8qMxERn1M3o4iIhLwwyGXqZhQRkdCnykxExMeMwPyMfqdkJiLicxrNKCIiEgJUmYmI+JmFxytgVJmJiEjIU2UmIuJzYVCYKZmJiPiZER6vgFE3o4iIhDwlMxERnwvMnH/hn6Sdy3Ka2UQzW2Nmv5lZHTPLbWYzzOxP789cXlszs6FmttbMVppZ1TjHucFr/6eZ3XC28yqZiYj4nHkjGi/0k0RvANOdc5cBlYDfgP7ATOdcaWCmtwzQAijtfW4F3vbizQ08BdQCagJPxSTAhCiZiYhIUJjZRUBD4H0A59xx59w+oA0w0ms2Emjr/dwGGOUCFgA5zewSoBkwwzm3xzm3F5gBNE/s3EpmIiI+FqwuRq8wy2tmS+J8bj3tdCWAncCHZrbczN4zs6zAxc65bQDen/m99oWATXH23+ytS2h9gjSaUUTE54I4mnGXc656ItsjgarAPc65hWb2Bv91KcYnvsBcIusTpMpMRESCZTOw2Tm30FueSCC5bfe6D/H+3BGnfZE4+xcGtiayPkFKZiIiPmdB+pyNc+4fYJOZlfVWNQZ+BSYDMSMSbwC+9H6eDPTyRjXWBvZ73ZDfAE3NLJc38KOpty5BCXYzejfyEgv6QOKXJSIiaUEKz814DzDGzDIA64HeBAqnCWbWB/gb6Oi1/Rq4FlgLHPba4pzbY2bPAYu9ds865/YkdtLE7pmt5sy+y5hlB1ya5EsTEZGw4JxbAcR3X61xPG0dcFcCx/kA+CCp500wmTnniiS0TUREQkNgOqvUjiL5JememZl1MbMB3s+Fzaxa8oYlIiJBEaQHptP6a2TOmszM7C3gaqCnt+ow8E5yBiUiInIukvKcWV3nXFUzWw6xN+YyJHNcIiISJGm8qAqKpHQznjCzCLwH1swsD3AyWaMSERE5B0mpzIYBk4B8ZvYM0Al4JlmjEhGRoEnr97uC4azJzDk3ysyWAk28VR2dc78kb1giIhIM4TKaMalzM6YDThDoatSsISIikqYkZTTjY8BYoCCB+bE+MbNHkzswEREJjnAYmp+UyqwHUM05dxjAzAYCS4EXkzMwEREJjrSdhoIjKV2Gf3Fq0oskMN+WiIhImpDYRMOvE7hHdhhYbWbfeMtNgTkpE56IiFwIs6C+zyzNSqybMWbE4mpgapz1C5IvHBERCbYwyGWJTjT8fkoGIiIicr7OOgDEzEoCA4HyQKaY9c65MskYl4iIBElaH4kYDEkZAPIR8CGBATEtgAnAuGSMSUREgsgsOJ+0LCnJLItz7hsA59w659zjBGbRFxERSROS8pzZMQvUqOvM7HZgC5A/ecMSEZFgMCzsRzPG6AtkA+4lcO8sB3BTcgYlIiJyLpIy0fBC78d/+e8FnSIiEgpC4H6eue/fAAAgAElEQVRXMCT20PTneO8wi49zrl2yRCQiIkEVDqMZE6vM3kqxKFJR2SK5GfVGt9QOQ3wgV427UzsEkbCV2EPTM1MyEBERSR7h8N6upL7PTEREQpARHt2M4ZCwRUTE55JcmZlZRufcseQMRkREgi/C/4VZkt40XdPMVgF/esuVzOzNZI9MRESCIsKC80nLktLNOBRoCewGcM79jKazEhGRNCQp3YwRzrm/TruBGJ1M8YiISBAFJglO42VVECQlmW0ys5qAM7N0wD3AH8kbloiIBEta7yIMhqR0M94BPABcCmwHanvrRERE0oSkzM24A+iSArGIiEgyCINexiS9aXoE8czR6Jy7NVkiEhGRoDHQK2A838X5ORNwPbApecIRERE5d0npZhwfd9nMPgZmJFtEIiISVOEw1dP5XGNxoGiwAxERETlfSblntpf/7plFAHuA/skZlIiIBE8Y3DJLPJlZ4Em7SsAWb9VJ51yCL+wUEZG0xczCYgBIot2MXuL63DkX7X2UyEREJM1Jyj2zRWZWNdkjERGRZBGY0urCP2lZgt2MZhbpnIsC6gO3mNk64BCBxxacc04JTkQkBITDdFaJ3TNbBFQF2qZQLCIiIuclsWRmAM65dSkUi4iIBJlmAIF8ZvZAQhudc4OTIR4REQmyMMhliSazdEA2vApNREQkrUosmW1zzj2bYpGIiEjwmQaAhMHli4j4n4XBP+eJPWfWOMWiEBERuQAJVmbOuT0pGYiIiARfYDRjakeR/JLyPjMREQlh4ZDMwuE1NyIi4nOqzEREfM7C4EEzVWYiIhLyVJmJiPiYBoCIiEjoC4HXtwSDuhlFRCTkqTITEfG5cJ81X0REQly43DNTN6OIiIQ8VWYiIj4XBr2MSmYiIv5mRIT5rPkiIiIhQZWZiIiPGepmFBGRUBcmb5pWN2OY2b51M3d0a0mnpjXp3Lw24z58O3bbH7+t4qYO19C1RV0euKUzB/89ELvto7cH0+7qKnRoUp35P82MXf/cI3fRrEYpujSvk6LXISISl5JZmEkXGcl9A55nwreL+GDiDD4d/R7r/1wDwMBH7+Xuh55i7LR5XNW0JaNHDAVg/Z9r+HbKJMZNX8AbH07k5af6ER0dDcB17bvxxocTU+16ROTsIsyC8knLlMzCTN78BbjsisoAZM2WneKlyrBz+zYA/t6wlio16wFQq97VfP/NVwD89N3XNG3ZngwZM1KoSDEKFy3B6p+XAlC1Zj0uypkrFa5EROQ/SmZhbOvmv/h99Sour1QNgBKly/HTd18D8N20L9i+bQsAO7dv4+JLCsXul79AwdgEKCJpW8wAkGB80jIlszB1+NBB+t/ZiweeeIFs2S8C4ImX3mLi6Pfo1fpKDh86SGT69AA4587YP63/YovIf8Khm1GjGcNQ1IkTPHJXL5q16cjVzVrHri9WsgxvjvwcgL82rGXu998CgUospkoD2PHPVvLmvyRlgxYRSYQqszDjnOO5/ndTvGQZuve5+5Rte3btBODkyZN88NYrtOvWG4AGjVvw7ZRJHD92jC2bNrJp47rYrkkRSfvCoZtRlVmY+XnpAqZ9MZ5SZcvTvWV9AO7s9yT1rm7Kt19N5NPR7wFwdbNWtOrQA4CSZcrR5Nrr6dy8FunSRfLw06+SLl06AB6/rw9LF85h397dtKxXnlvu60+bTr1S5+JE5AxGeFQtFt/9kHBSrkIVN+rLH1I7DPGBhu0fS+0QxCeOrhi21DlXPRjHKl6uontq1JRgHIreNYsGLa5gU2UmIuJnBpbW+wiDQMlMRMTn/J/KwqMrVUREfE7JLI1q07ACXVvUpXvL+nRvWZ+VSxcm2v7KCoUS3Z4Uzzx0B9fVLcfxY8cA2LdnN20aVrjg457uh2+nxE6hBfDu6wNZNPeHoJ9HktddXa9iyacDWDrxMe7udlXs+gplCvHDyH4snjCAiUNuI3vWTABUv7woC8b1Z8G4/iwc35/WV1cEoPDFOZk+/F6WT3qcpRMf466uV8VzNjlfhp4zk1T29pivyJk7T4qeMyIiHZMnjqZD9z7Jdo4fZ0ylfqPmlCh9GQC39dXAiVBTvuQl9G5XlwY9X+H4iWgmD7uTaXNWs+7vnbz9ZDf6v/45c5aupVeb2vS9oTHP/m8qq9dtpV73l4mOPkmBvBexcPyjTP3pF6KiT9J/8GesWLOZbFkyMu+TR5i5cA1r1v+T2pfpGymdhswsHbAE2OKca2lmxYFxQG5gGdDTOXfczDICo4BqwG6gs3Nuo3eMR4E+QDRwr3Pum8TOqcoshBw+dJA7e7SmZ+uGdG1Rlx9nTD2jza4d/3BrlxZ0b1mfLs3rsHzxPAAWzJ7FTR2uoWfrhvS/+wYOHzoY7zm69L6dsR/8j6ioqDO2fTx8KDe0vZpu19Zl+JAXYte//+bLdLymBnf3asvj9/Vh9Ig3Afhi3MhA++vq8cidPTl65DArly5k9sxpDB30BN1b1mfzXxt45qE7mDntS+b9MINH77kx9rhLF8zmgVs6n1P8kjIuK16ARas2cuToCaKjTzJ76VraXF0JgNJF8zNn6VoAZi1YQ9vGgblAY9oCZMyQPnZmmX92HWDFms0AHDx8jDUb/qFgvpwpfUkSXPcBv8VZfgl43TlXGthLIEnh/bnXOVcKeN1rh5mVB7oAlwPNgf95CTJBSmZp2B3dW9G9ZX16t2sMQIaMmXj57dF8PPkn3h7zFW+88PgZU019M/lTajdozJgpcxgzdQ5lylVg357dfDDsFYaN+oKPJ/9EuQpV+OSDYfGes0DBIlSuXptpX4w7Zf2C2bPYtHEdH30+i9FT5vDbLz+zbNFcfl25nFnfTObjr37ipf99zG+rlsfuc1WzVoz84ns+mTqXYiXL8uWEj6lYrRYNGrfg3v7PMWbKHAoXLR7bvmb9q/ll+RKOHD4EwIypn3PNde3OKX5JGavXbaV+1VLkzpGVzJnS07z+5RQuEJhw+td122h5VaB7ut01VSl88X8TUde4oihLJz7Gkk8HcO/AcbHJLcall+SmctnCLP5lY4pdSzhIyYemzawwcB3wnrdsQCMg5vUaI4G23s9tvGW87Y299m2Acc65Y865DcBaoGZi51U3Yxp2Rjejc7z92nMsXzQXi4hg5/Zt7N61g7z5Lo5tUq5iVZ5/5G6iok5w1TXXUaZ8RWbPms6Gtb9zc6dmQGA6qyuq1EjwvDfe0Y8Hb+tKvauaxa5bOHsWC+fMokerBgAcOXSITRvXcfjQQa5sci2ZMmUGoH7j5rH7rP/jV94ePJCDB/Zz+PBBajdonOj1RkZGUqdhY2bPnE6jFm2Y+/233PPIMyxbNPec4pfk9/uG7bz20QymvH03h44cY+UfW4iKCrwW6Lanx/Dawx149JYWTP1xFcdPRMfut/iXv6jWYSBli1/Me8/25Ju5v3LseKAXIGvmDIx99WYeenUS/x46mirX5U+W0kPzhwAPA9m95TzAPudcTHfPZiDmJn8hYBOAcy7KzPZ77QsBC+IcM+4+8VIyCyHTv5zA3t27GPXlj0SmT0+bhhU4fuzU/+mr1qzHu+O+Zu733/JUv9vpccs9XJQjJ7XqXc3zb7yfpPMUKVaC0uUq8N3Xn8euczhuuP2B2CmuYiRWIT3z8J288s4YypSrwJSJY1i6cM5Zz93kunZMHD2Ci3LmonzFKmTNlh2cO6f4JWWM/GI+I7+YD8Azd7diy/Z9APyxcTut7gz8XpS6ND8tGlx+xr6/b9jOoSPHubxUQZb9+jeRkRGMffUWxk9bwpezfk65i5BzldfMlsRZHu6cGx6zYGYtgR3OuaVmdlXM6niO486yLbF94qVuxhBy8N8D5MqTl8j06Vky/ye2bdl0RpttW/4mV558tO1yA6079eD31T9zReUa/Lx0IZs2rgfg6JHD/LVhbaLn6n1nP8a892bscu0Gjflq4ujYe1U7/tnKnl07qVy9DrNnTefYsaMcPnQwdnJiCNzjy5uvAFEnTjB98qex67NkzcbhQ//Ge95qtevz++qf+WL8SJpc1w7gvOKX5JcvVzYAihTIRZtGlZgwfckp682M/rc0Y8TEwJeYogXzkC5d4J+cSy/JRZliF/PX1t0AvPNUd37f8A9DR89K6cvwvZjprILxAXY556rH+QznVPWA1ma2kcCAj0YEKrWcZhZTPBUGtno/bwaKAHjbcwB74q6PZ594qTILIc3bdOKBW7vQq81VlClfgWIly5zRZumCOYwe8SaR6SPJnCUbT7/6Nrny5OXJl4fx+P19OHE8MOz+9gcep2jxUgmeq2SZcpS9vBK/rw58S67doBEb1/5Onw5NAcicNSvPvjac8hWr0qBxC7pfV59LChWhXIUqsa+Uua3vY/Ru35hLChWhZJnysYmwacv2vPDYfYwf+S6D3hp1ynnTpUtH/UbNmTLpE55+5W2A84pfkt/YV28md86snIiK5v5BE9j37xEAOjWvzm2dGwLw5awVjPoy0FtUt0oJHuzdlBNR0Zw86bjvhfHs3neIupVL0L1lLVb9sYUF4/oD8NRbk/lmzq+pc2Fy3pxzjwKPAniV2YPOue5m9inQgUCCuwH40ttlsrc839s+yznnzGwy8ImZDQYKAqWBRYmdW3Mzam7GC3b40EGyZM3G0SOHubXLtQwYOCT2bdbhRHMzSrAEc27GkuUruRc/mRaMQ9G5SqEkxxUnmbU0sxL8NzR/OdDDOXfMzDIBHwNVCFRkXZxz6739HwNuAqKA+51ziV6EKjO5YC88dj8b1q7h+LFjXNeua1gmMpG0LDUed3bO/QD84P28nnhGIzrnjgIdE9h/IDAwqedTMpML9vyQ91I7BBEJc0pmIap3u8YcP36MA/v2cezYEfJdHHjz8yvvjKFg4aJBP9/brz1Pzty56dr7zjPWT5k05pRHCIaPmxYYhShp1k+jHiRDhkhyX5SFTJnSs3XHfgA69R3O39v2BO08JYrkZcmEAfzx1w4ypE/Hj4v/pO+gCed8nMnD7qLbQ++RPjId7ZtW5T1vUEnhi3PyYt/r6dn/w6DF7DuaNV/Ssg8/mwnAlIlj+O2XFTz09CupFkuPW+45I8nFFRUVRWRkZILLCXHO4ZwjIkKDboOtYa9XAejRqhbVyl9K35c+jbddRIRx8uSF3Vf/468d1O4yiMjICL4dcR/XXVmBqT+uOqdjtL4rMNS/RJG83Nyhfmwy27x9nxLZWYTLyznD4RrDymeffMjQF5+IXZ445n2GDnqSTRvX07l5bZ7qdytdW9Tl0Xtu5OjRwOizX1cu47au19Kr9ZXc17sDu3ftuOA4vhg/isfuvYm+N3fm/ps6sGjuD9zVsw2P3XsTPVsFRrqNevcNujSvQ5fmdRg/8l0ANm1cT5fmdXjx8b70bN2QXTs0P19KSpcugm0/vcxTd7Zk9scPUuOKYqyd/hw5sgUeiq9ZoRhT37kbCDzkPPyZHsz++EHmj32Eaxtekeixo6JOsnDlRkoWyYeZ8VK/diz5dACLJwzg+iaB+6wF8+Vg5gd9WTCuP0s+HUDtSoEZYmJieP7eNpQpmp8F4/rz3L2tKVEkb+wIyDljHqZ00fyx55v5QV8qlCl0znFKaFIy85lmrTvw/bdfxc6tOGXiGFq27wbAhj/X0KHHLYydNo8MGTLy+dgPOX7sGK8925+Xhn3MqMk/0qJtJ94dnOR7rgCMHvFm7Oz+d/VsE7t+1fJFPPPau7w16gsAflmxhHv6P8PYafNY/fNSpk+ewEefz+T9id8yacz7/Lnml0Cca9fQplNPRn81m/wFCgbjr0XOQc7sWVixZhMNer7KwpUbEmw34NYWzJj3Gw16vkqLW4cy6IF2ZMyQcMWdJVMGrqxRhl/+3Er7a6pwWfEC1Oz8Ii3veJOX+7UnX65sdL2uBl//tIraXQZRs/OLrPpjyynHeHzol7GV3hNDJ5+ybdI3S2nftCoAhfLnJHeOrKz6Y8s5x+lHZhaUT1qW4v9Fzex64DOgnHNujZkVA+o65z7xtlcGCjrnvj7P428EqjvndgUn4tCSNVt2qtSoy/wfZ1CwSDEi0qWjROnL2LRxPQWLFKWCNw1Ui7ad+WLcR1SrVZ/1f67hrl6BJHQy+uQ5J5CEuhlr1W/ERTn+mzC2QpUaFCgYeA5yxeJ5NGremkyZswBw5TXX8fOSBdSq34jClxanfMWq53X9cuGOHT+RpFk4GtcpR9N6l9Ov9zUAZMoQSZECuVn796mVfUwldfKkY/L3PzNr4RoGP9KR8dOXcPKkY/vuf5m3Yh1VL7+UJav/5q3Hu5AxQ3q++mHlGcksMZNmLGPikNsZNGI6HZpVZdKMZecUp5+l7TQUHKnx9aQrMIfAjMhPA8WAbsAn3vbKQHXgvJKZQJtOvfjkg2FcUvhSWrXvHrv+jG9WZjjnKHXZ5YwYH5znUOLKnCXLKcsxiQsgsccbM522n6SsI8dOnLIcFX2SiIjA707GDOlj15tBpweGs2Fz4t8bYyqpuBL6x/XHxX/Q7OY3aN7gCj4ceAOvfvAt46YtSaD1qf7etpdDR45xWYkCdGhalVueGn1OcUpoS9FuRjPLRmC6kz4EkhnAIKCBma0ws0eAZ4HO3nJnM6tpZvPMbLn3Z1nvWOnM7FUzW2VmK83sntPOldnMppvZLSl4iWlCpeq12fz3BmZ+/SXXtLw+dv3WTX/x68rAt9Vvv5pI5Wq1KV7qMnZu38rqn5cCcOL4cdb98Vu8xw2mKjXr8sO3Uzh69AiHDx3kp+++pnKNOsl+Xjl3f23dQ5VylwLE3tsC+G7eb9zV9crY5UplCyf5mHOWraVjs2pERBj5c2enTqUSLFv9N5dekot/dh/gg8/mMnryQipdVuSU/Q4eOkb2LBkTPO7Eb5bxUO+mZMgQGfs+tAuJ0y9Sctb81JLSlVlbYLpz7g8z22NmVYH+eE+JA5jZdgLdhHd7yxcBDb0ZlZsALwDtgVuB4kAVb1vuOOfJRuBp81HOuVPnSwoTjZq34a/1f5Ite47YdSVKl+OLcSN5vv/dFC1ZhrZdbyRDxowMemskrz3bn0OH/iU6Oprufe6iZJlyST7X6BFvMmXSJ7HLg4ePS6R1wOWVqtG0ZQdubNsIgHbdbqJU2ctj51+UtOP5d77mf092ZfuuAyxZ/Vfs+oHvTuOVh9qzeMIAIiKMdZt20qnv6VP1xe+z71ZQs2JxFo1/FOfgkcGfsXPvQXq1qc29PRpxIiqaQ4ePcdPjI0/Zb8eef1n2698snjCA6XN+4cPP55123OW81K8dz77937v+LiROPwiMZkzjmSgIUnQ6KzObCgxxzs0ws3sJTCQ5lVOT2Y2cmsyKAEMJzM3lgPTOucvMbBLwjnNuxmnn2AjsB152zo1JII5bCSRDChQsUm3y7HMbJhwK7r2xPTfe0ZeqteoDgVGC/e/uxZgpZ5+5Xs6PprOSYAnmdFalL6/kBo/79uwNk6B1xQJBiyvYUqyb0czyEJhB+T0v4TwEdObs9yafA753zl0BtAIyxRyShF8JMBdoYQkMv3HODY+Z9fmU94X5wL69e2jfuBoX5cgZm8hEJLypmzG4OhDo9rstZoWZ/Qic5L+XuAH8e9pyDiBmSNONcdZ/C9xuZj/EdDM652KmLngSeAL4H3BHUK8ijcuZKzeTZi49Y32RYiVUlYmEJcPCoJsxJQeAdAU+P23dJAIDQaLM7Gcz6wt8D5SPGQACvAy8aGZzgXRx9n0P+BtYaWY/ExgRGdf9QCYzezkZrkVERNKQFKvMnHNXxbNuaALNa5y2HPfFXU94+0YBD3ifuMcsFmfx1Ncii4iEobTeRRgMmgFERERCXnjN6SIiEmbCZWi+kpmIiJ+FwEjEYFA3o4iIhDxVZiIiPhcOlZmSmYiIz+k5MxERkRCgykxExMcMiPB/YaZkJiLid+pmFBERCQGqzEREfE6jGUVEJOSpm1FERCQEqDITEfGxcBnNqMpMRERCniozERFfC483TSuZiYj4mWbNFxERCQ2qzEREfC4MCjMlMxERPwuMZvR/OlM3o4iIhDxVZiIiPuf/ukzJTETE/8Igm6mbUUREQp4qMxERn9ND0yIiEvLCYDCjuhlFRCT0qTITEfG5MCjMlMxERHwvDLKZuhlFRCTkqTITEfExIzxGM6oyExGRkKfKTETEz8LkfWZKZiIiPhcGuUzdjCIiEvpUmYmI+F0YlGZKZiIivmYazSgiIhIKVJmJiPicRjOKiEhIM8Lilpm6GUVEJPSpMhMR8bswKM2UzEREfE6jGUVEREKAKjMREZ/TaEYREQl5YZDL1M0oIiKhT5WZiIifhcmDZqrMREQk5KkyExHxuXAYmq9kJiLiY0Z4jGZUN6OIiIQ8VWYiIj4XBoWZkpmIiO+FQTZTN6OIiIQ8VWYiIj6n0YwiIhLyNJpRREQkBKgyExHxuTAozJTMRER8LwyymboZRUQk5KkyExHxscCk+f4vzVSZiYhIyFNlJiLiZ6ah+SIi4gMWpM9Zz2NWxMy+N7PfzGy1md3nrc9tZjPM7E/vz1zeejOzoWa21sxWmlnVOMe6wWv/p5ndcLZzK5mJiEiwRAH9nHPlgNrAXWZWHugPzHTOlQZmessALYDS3udW4G0IJD/gKaAWUBN4KiYBJkTJTETE71KoNHPObXPOLfN+/hf4DSgEtAFGes1GAm29n9sAo1zAAiCnmV0CNANmOOf2OOf2AjOA5omdW/fMRER8zVJlNKOZFQOqAAuBi51z2yCQ8Mwsv9esELApzm6bvXUJrU9Q2CezNb+s2FWzZM6/UjuOEJAX2JXaQYgv6Hfp7IqmdgAJyGtmS+IsD3fODT+9kZllAyYB9zvnDljCI1Di2+ASWZ+gsE9mzrl8qR1DKDCzJc656qkdh4Q+/S6lvCCOZtx1tv92ZpaeQCIb45z7zFu93cwu8aqyS4Ad3vrNQJE4uxcGtnrrrzpt/Q+JnVf3zEREfCxYt8uSOJrRgPeB35xzg+NsmgzEjEi8Afgyzvpe3qjG2sB+rzvyG6CpmeXyBn409dYlKOwrMxERCZp6QE9glZmt8NYNAAYBE8ysD/A30NHb9jVwLbAWOAz0BnDO7TGz54DFXrtnnXN7Ejuxkpkk1Rn94iLnSb9LKS2Fxn845+YkcrbG8bR3wF0JHOsD4IOknlvJTJIkvpu8IudDv0spT3MzioiIhABVZiIiPhcOczMqmYmI+FwY5DJ1M8qFM7NyZtbIe75E5JxZIk/ViiSFKjMJhi4EHnyMNrN5zrkTqR2QhBZvVBtmVgfY4Jz7J5VD8g+9AkYkyZ4BNgKdgfqq0CSpzKyKmWXwfi4FPE9g5nWRc6JkJuclbreQc+4kMBDYhhKanJunga+8hLYO2A8cBzCzCDNLl4qx+UhKzQGSepTM5JyZmcXpFmpqZlcBOQl8q/6bQEKrq4QmCTGzCADnXBtgLzAByE6gws/ibTsJZEilEH3DCHQzBuOTlumemZyzOInsAeB64FfgFuA959wLZvYIgRftRQNzUi1QSZO8L0MnvZ/zOee6mNmXwHwCvzMFzSyKQCLbamaPOueOpGLIEgKUzOS8mFkT4GrnXAMze5HA22C7mhnOuZfMrC+B+dZEThHny9C9QHUzu8M518bM3gEaAS8R6DXKBfyuRHbh0nhRFRRKZpIkcbsWPZuAe8zsRqAGgclCXweeNrP0zrnXUyFMCRFm1hboBbRyzh0CcM7dbmafAs8BbZ1zGggSJGm9izAYdM9Mzuq0e2S1vFcybHDObQRKA297r21YDvzsfUQSUxKY4r3fKn3M/VXnXEdgO1AwVaOTkKPKTM4qTiK7HXgIWA18a2bjgF+AkWZWFWgJXO+c255qwUqaE09VD7AFaGhmFznnDnjtOgGbnXN9UjxInwuHiYaVzCRBp1Vk+YGKBO6NVQeuAfoAbxEYTl0L6OKcW59K4UoadNrvUDvgX+AggRct9gB6m9nvBO6PPQa0Sq1Yfc3/uUzJTOJ32j9CdxHo9rncObcb+MYbWt0EeBh4wzn3depFK2nVaYM9uhF4l9nDwJ0ERsDeQ+CLUCagq3NuQyqFKiFO98wkXnH+EWoDdAUWAoXMbLy3fRrwE5CesPjeJ+fLzKoArYGrgMLADuA9oJZzboBzrhvQyzm3KvWi9Df/PzKtZCaniTuzh5lVJzDi7CPn3GQCgz3KmNlYAOfcl8BAr1oTAcDMcppZSe/nisARAlVZW+Aa51xDYAQw3sx6ADjnDqZWvH4XrAem0/qISHUzSqx47m9cTmB2hqvNbLFz7mdvoMd6M/vIOXdjzLBqEQAziwTKAC3N7BIgL9DdOXfYzHICn3hNdwKDgQWpE6n4jZKZxIqTyBoRGNzRFihH4EZ9azM76XUFFTez4qkXqaRF3pehKG9AxwCgDvCwc+6w1yQSaGZmZQncb23mnNuSSuGGlXAYzahuRjmFN8/iHcBK59wJ59xK4EsgK9DNzC4H0I16ict79rC5t1iGwByLw4CqZtYKwDn3FvAZsBLorESWgsLgppkqszAXzzNAG4A9QGkzq+icW+mcm+s91NqIwAOtIqdLD9QzsycBnHN1zCwvgXtlrcxsH5COwIz4Y2PmZhQJFiWzMHbaPbJWBN4jtQ+4G3gD6OQ1+X97dx8jV1WHcfz70IC2tFBfYkkQKfIiNpWWNg0V34g2xQZENBCtiFZKKWtiFKUpCZCoMRHDHybYVMQ3FAygURAkpGlRoZAW0bWlKG1BCDHy1jWmCAFM8PGPczYZ19LuwsjMnft8NpPMzj17z5nNZn9z7j3n99tm+7eS7kmevOgk6RDbT9h+StKTwCzK7AvbI5JuofxdrQbmAB9IIHv19fmkqitymTGQ9Fngq8C7gR8AF9THdGCZpFkACWTRSdKxlKz235S0FLgS+DCwS9La+mHpEWA9cA6w0PbOHg45BliCWQtJeoukA227ZvY4E1wvvskAAAVVSURBVPiE7YuBE4GVwBmUgpuTKPuCIsZ6llK25QngXGAtcBAlu8fTwBpJZ1M+GD2de2S904al+QlmLSNpBvAlYEjSVNtPASPU6r62/0H553NcTR68yvZIzwYcfcv2X4HfAfOAk4HfAGdTst7fArweWAassf18j4YZqGtf/SzBrH12AfdS0lN9pm6Sfhi4vu4RAjgceHMtWZ8yHPE/OjbXrwZM2U/2GCVv5zZKmqq/AZ+2/eeeDDJaJQtAWkLS0cB+tndI+gklOfASYIXt1ZK+Ddwp6T5KrryzbL/YwyFHH6uXqEcXbD9E2QA9D7jA9k31ftqTdaYfPST6/xJhNySYtYCkNwA7gBFJX6GUpr8KOBg4StJK20OSTgAmA9/IPrLYl7oS9gVJ1wAbgW/Zvqke297TwUXrJJi1gO2/S1oEbKBcWp4D3EApxfEv4B31U/YPbb/Qu5FGE9XZ/mrgcElTOjJ+RLxqEsxawvavJZ0MXEEJZjMom6A/TqlR9jbgOiDBLF6OTcBHez2I2LNcZoyBYnu9pAsp1aEX2v6RpJsp2Rum2N7d2xFGU9neLuljmZX1p35fidgNCWYtY/tWSf8GNkt6Z8q3RLckkEUvJZi1kO3bJB0AbJA0P+mFIgZYAzY8d0OCWUvZ/qWk2xPIIgZbAxLed0U2TbdYqvtGxKDIzCwiYtC1YGqWmVlERDReZmYREQOuDUvzMzOLxpL0oqQtku6X9DNJU17BuU6S9Kv6/DRJF+2l7fRaA26ifXy57vMb1+tj2lwt6YwJ9DVT0v0THWMMppSAiehvz9mea3s2JS3X+Z0HVUz4b9z2zbYv20uT6cCEg1lE/P8kmMWg2EhJmjxT0gOS1gLDwGGSFkvaJGm4zuCmAkj6oKTtku6iIxWTpGWS1tTnMyTdKGlrfZwIXAYcWWeFl9d2qyTdK+m+msx59FwXS9ohaQMlZdheSVpRz7NV0s/HzDYXSdooaaekU2v7SZIu7+h75Sv9RcbgUZce/SzBLBqv1mFbQqmjBSVo/Nj28ZRqyJcAi2zPA34PfFHSa4HvAh8C3gMc8hKnvwK4w/YcSomTPwEXAX+ps8JVkhYDR1NyXM4F5kt6r6T5lNyXx1OC5YJxvJ1f2F5Q+3sAWN5xbCbwPuAU4Mr6HpYDu20vqOdfIemIcfQTbdKCaJYFINFkkyVtqc83At+nFB191Pbm+vpCYBZwd60neQAlKe6xwCO2HwSQdC1w3h76eD/wKYBa3223pNeNabO4Pv5Yv59KCW7TgBtH0zzVPJj7MlvS1yiXMqcC6zqO/bRucn9Q0sP1PSwGjuu4n3Zw7XvnOPqKGBgJZtFkz9me2/lCDVjPdr4ErLe9dEy7uZQKyd0g4Ou2vzOmjy+8jD6uBk63vVXSMuCkjmNjz+Xa9+dsdwY9JM2cYL8xwLKaMaL5NgPvknQUgKQpko4BtgNHSDqytlv6Ej9/OzBUf3aSpIOAf1JmXaPWAed03Is7VNKbgDuBj0iaLGka5ZLmvkwDHpe0P3DWmGNnStqvjvmtlIKr64Ch2h5Jx0g6cBz9REuMVpoe9NWMmZnFQLO9q85wrpP0mvryJbZ3SjoPuFXSCHAXMHsPp/g8cJWk5ZQK3UO2N0m6uy59v63eN3s7sKnODJ8BPml7WNINwBbgUcql0H25FLintt/GfwfNHcAdlFp059t+XtL3KPfShmuB1V3A6eP77UQbDA//Yd3k/fXGLp1upEvn6TqVyucRERHNlcuMERHReAlmERHReAlmERHReAlmERHReAlmERHReAlmERHReAlmERHReAlmERHReAlmERHReP8BvRfAGeDF0pAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x67b9208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d627e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
