{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 3\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import sklearn.metrics as me \n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    \n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_, \"Actual\": actual_value_})\n",
    "                        \n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred_, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        print(\"********************************** Training ******************************\")\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        Train.predictions = {}\n",
    "        Train.predictions_ = {}\n",
    "\n",
    "        Train.results = []\n",
    "    \n",
    "        \n",
    "        features_arr = [30]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-5]\n",
    "        print(\"********************************** Entering Loop ******************************\")\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions_.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "        \n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_dense_only_nsl_kdd_scores_allz5.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allz5.pkl\")\n",
    "            past_scores = past_scores.append(df_results, ignore_index=True)\n",
    "        past_scores.to_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allz5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************** Training ******************************\n",
      "********************************** Entering Loop ******************************\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:30\n",
      "Step 1 | Training Loss: 0.751610 | Validation Accuracy: 0.332116\n",
      "Accuracy on Test data: 0.3979772925376892, 0.5394092798233032\n",
      "Step 2 | Training Loss: 0.692577 | Validation Accuracy: 0.606049\n",
      "Accuracy on Test data: 0.6043736934661865, 0.5576371550559998\n",
      "Step 3 | Training Loss: 0.637426 | Validation Accuracy: 0.780997\n",
      "Accuracy on Test data: 0.7254258394241333, 0.5261603593826294\n",
      "Step 4 | Training Loss: 0.584383 | Validation Accuracy: 0.821241\n",
      "Accuracy on Test data: 0.7362934947013855, 0.5363712906837463\n",
      "Step 5 | Training Loss: 0.555641 | Validation Accuracy: 0.847436\n",
      "Accuracy on Test data: 0.7529719471931458, 0.5661603212356567\n",
      "Step 6 | Training Loss: 0.514334 | Validation Accuracy: 0.911097\n",
      "Accuracy on Test data: 0.7950230836868286, 0.6213502287864685\n",
      "Step 7 | Training Loss: 0.506521 | Validation Accuracy: 0.931100\n",
      "Accuracy on Test data: 0.8025638461112976, 0.6331645846366882\n",
      "Step 8 | Training Loss: 0.473752 | Validation Accuracy: 0.935069\n",
      "Accuracy on Test data: 0.8042494654655457, 0.6358649730682373\n",
      "Step 9 | Training Loss: 0.450552 | Validation Accuracy: 0.948881\n",
      "Accuracy on Test data: 0.8034954071044922, 0.6330801844596863\n",
      "Step 10 | Training Loss: 0.452109 | Validation Accuracy: 0.953088\n",
      "Accuracy on Test data: 0.803584098815918, 0.6328269839286804\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:30\n",
      "Step 1 | Training Loss: 0.714603 | Validation Accuracy: 0.566360\n",
      "Accuracy on Test data: 0.454355925321579, 0.22641350328922272\n",
      "Step 2 | Training Loss: 0.700601 | Validation Accuracy: 0.702889\n",
      "Accuracy on Test data: 0.6004701852798462, 0.27485233545303345\n",
      "Step 3 | Training Loss: 0.677005 | Validation Accuracy: 0.813145\n",
      "Accuracy on Test data: 0.6521469354629517, 0.3534177243709564\n",
      "Step 4 | Training Loss: 0.661250 | Validation Accuracy: 0.857041\n",
      "Accuracy on Test data: 0.7113201022148132, 0.4613502025604248\n",
      "Step 5 | Training Loss: 0.637338 | Validation Accuracy: 0.889903\n",
      "Accuracy on Test data: 0.7380234003067017, 0.5114768147468567\n",
      "Step 6 | Training Loss: 0.592737 | Validation Accuracy: 0.916812\n",
      "Accuracy on Test data: 0.7630855441093445, 0.5583966374397278\n",
      "Step 7 | Training Loss: 0.589462 | Validation Accuracy: 0.928560\n",
      "Accuracy on Test data: 0.7889904379844666, 0.6054852604866028\n",
      "Step 8 | Training Loss: 0.567181 | Validation Accuracy: 0.933720\n",
      "Accuracy on Test data: 0.7976845502853394, 0.6200000047683716\n",
      "Step 9 | Training Loss: 0.557876 | Validation Accuracy: 0.933878\n",
      "Accuracy on Test data: 0.8100603222846985, 0.6422784924507141\n",
      "Step 10 | Training Loss: 0.525268 | Validation Accuracy: 0.936418\n",
      "Accuracy on Test data: 0.8187100887298584, 0.6578059196472168\n"
     ]
    }
   ],
   "source": [
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsz5.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsz5__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_scoresz5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Negative\", \"\\n False Positive \\n Type II Error\"],\n",
    "             [\"\\n False Negative \\n Type I Error\", \"\\n True Positive\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_dense_only_nsl_kdd_scores_allz5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.936418</td>\n",
       "      <td>0.818710</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.657806</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>34.008966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933878</td>\n",
       "      <td>0.810060</td>\n",
       "      <td>0.806385</td>\n",
       "      <td>0.642278</td>\n",
       "      <td>0.732369</td>\n",
       "      <td>30.681355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.935069</td>\n",
       "      <td>0.804249</td>\n",
       "      <td>0.800902</td>\n",
       "      <td>0.635865</td>\n",
       "      <td>0.727227</td>\n",
       "      <td>19.731505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.931100</td>\n",
       "      <td>0.802564</td>\n",
       "      <td>0.798988</td>\n",
       "      <td>0.633165</td>\n",
       "      <td>0.724786</td>\n",
       "      <td>17.050687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911097</td>\n",
       "      <td>0.795023</td>\n",
       "      <td>0.791198</td>\n",
       "      <td>0.621350</td>\n",
       "      <td>0.715346</td>\n",
       "      <td>14.801043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933720</td>\n",
       "      <td>0.797685</td>\n",
       "      <td>0.791115</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.710697</td>\n",
       "      <td>27.389075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.928560</td>\n",
       "      <td>0.788990</td>\n",
       "      <td>0.779329</td>\n",
       "      <td>0.605485</td>\n",
       "      <td>0.694225</td>\n",
       "      <td>24.248975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847436</td>\n",
       "      <td>0.752972</td>\n",
       "      <td>0.756291</td>\n",
       "      <td>0.566160</td>\n",
       "      <td>0.682713</td>\n",
       "      <td>12.547546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821241</td>\n",
       "      <td>0.736293</td>\n",
       "      <td>0.744839</td>\n",
       "      <td>0.536371</td>\n",
       "      <td>0.669633</td>\n",
       "      <td>10.197279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.916812</td>\n",
       "      <td>0.763086</td>\n",
       "      <td>0.744560</td>\n",
       "      <td>0.558397</td>\n",
       "      <td>0.643164</td>\n",
       "      <td>20.553508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780997</td>\n",
       "      <td>0.725426</td>\n",
       "      <td>0.737578</td>\n",
       "      <td>0.526160</td>\n",
       "      <td>0.665993</td>\n",
       "      <td>7.500255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.889903</td>\n",
       "      <td>0.738023</td>\n",
       "      <td>0.708173</td>\n",
       "      <td>0.511477</td>\n",
       "      <td>0.586943</td>\n",
       "      <td>17.145405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606049</td>\n",
       "      <td>0.604374</td>\n",
       "      <td>0.679530</td>\n",
       "      <td>0.557637</td>\n",
       "      <td>0.708389</td>\n",
       "      <td>4.895687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857041</td>\n",
       "      <td>0.711320</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>0.520039</td>\n",
       "      <td>13.780243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.813145</td>\n",
       "      <td>0.652147</td>\n",
       "      <td>0.563363</td>\n",
       "      <td>0.353418</td>\n",
       "      <td>0.353963</td>\n",
       "      <td>10.659430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332116</td>\n",
       "      <td>0.397977</td>\n",
       "      <td>0.562081</td>\n",
       "      <td>0.539409</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>2.305003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.702889</td>\n",
       "      <td>0.600470</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.274852</td>\n",
       "      <td>0.207945</td>\n",
       "      <td>6.883731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.566360</td>\n",
       "      <td>0.454356</td>\n",
       "      <td>0.086175</td>\n",
       "      <td>0.226414</td>\n",
       "      <td>0.105746</td>\n",
       "      <td>3.551186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "17     11              30              3     0.936418    0.818710  0.816817   \n",
       "16     10              30              3     0.933878    0.810060  0.806385   \n",
       "7       9              30              1     0.935069    0.804249  0.800902   \n",
       "6       8              30              1     0.931100    0.802564  0.798988   \n",
       "5       7              30              1     0.911097    0.795023  0.791198   \n",
       "15      9              30              3     0.933720    0.797685  0.791115   \n",
       "14      8              30              3     0.928560    0.788990  0.779329   \n",
       "4       6              30              1     0.847436    0.752972  0.756291   \n",
       "3       5              30              1     0.821241    0.736293  0.744839   \n",
       "13      7              30              3     0.916812    0.763086  0.744560   \n",
       "2       4              30              1     0.780997    0.725426  0.737578   \n",
       "12      6              30              3     0.889903    0.738023  0.708173   \n",
       "1       3              30              1     0.606049    0.604374  0.679530   \n",
       "11      5              30              3     0.857041    0.711320  0.665811   \n",
       "10      4              30              3     0.813145    0.652147  0.563363   \n",
       "0       2              30              1     0.332116    0.397977  0.562081   \n",
       "9       3              30              3     0.702889    0.600470  0.461400   \n",
       "8       2              30              3     0.566360    0.454356  0.086175   \n",
       "\n",
       "    test_score_20  f1_score_20  time_taken  \n",
       "17       0.657806     0.747021   34.008966  \n",
       "16       0.642278     0.732369   30.681355  \n",
       "7        0.635865     0.727227   19.731505  \n",
       "6        0.633165     0.724786   17.050687  \n",
       "5        0.621350     0.715346   14.801043  \n",
       "15       0.620000     0.710697   27.389075  \n",
       "14       0.605485     0.694225   24.248975  \n",
       "4        0.566160     0.682713   12.547546  \n",
       "3        0.536371     0.669633   10.197279  \n",
       "13       0.558397     0.643164   20.553508  \n",
       "2        0.526160     0.665993    7.500255  \n",
       "12       0.511477     0.586943   17.145405  \n",
       "1        0.557637     0.708389    4.895687  \n",
       "11       0.461350     0.520039   13.780243  \n",
       "10       0.353418     0.353963   10.659430  \n",
       "0        0.539409     0.696609    2.305003  \n",
       "9        0.274852     0.207945    6.883731  \n",
       "8        0.226414     0.105746    3.551186  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.936418</td>\n",
       "      <td>0.818710</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.657806</td>\n",
       "      <td>0.747021</td>\n",
       "      <td>34.008966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.935069</td>\n",
       "      <td>0.804249</td>\n",
       "      <td>0.800902</td>\n",
       "      <td>0.635865</td>\n",
       "      <td>0.727227</td>\n",
       "      <td>19.731505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "30             3                 11     0.936418    0.818710  0.816817   \n",
       "               1                  9     0.935069    0.804249  0.800902   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "30             3                   0.657806     0.747021   34.008966  \n",
       "               1                   0.635865     0.727227   19.731505  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='test_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>3</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.847873</td>\n",
       "      <td>0.713485</td>\n",
       "      <td>0.642313</td>\n",
       "      <td>0.491148</td>\n",
       "      <td>0.530211</td>\n",
       "      <td>18.890188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.770638</td>\n",
       "      <td>0.702360</td>\n",
       "      <td>0.733926</td>\n",
       "      <td>0.577015</td>\n",
       "      <td>0.698837</td>\n",
       "      <td>11.128626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "30             3                6.5     0.847873    0.713485  0.642313   \n",
       "               1                5.5     0.770638    0.702360  0.733926   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "30             3                   0.491148     0.530211   18.890188  \n",
       "               1                   0.577015     0.698837   11.128626  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.predictions = pd.read_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsz5.pkl\")\n",
    "Train.predictions_ = pd.read_pickle(\"dataset/FCN/tf_dense_only_nsl_kdd_predictionsz5__.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.panel.Panel'>\n",
       "Dimensions: 18 (items) x 22544 (major_axis) x 4 (minor_axis)\n",
       "Items axis: 10_30_3 to 9_30_3\n",
       "Major_axis axis: 0 to 22543\n",
       "Minor_axis axis: Actual to Prediction"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.133683</td>\n",
       "      <td>0.866317</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139095</td>\n",
       "      <td>0.860905</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.652066</td>\n",
       "      <td>0.347934</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374286</td>\n",
       "      <td>0.625714</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316657</td>\n",
       "      <td>0.683343</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900690</td>\n",
       "      <td>0.099310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.662228</td>\n",
       "      <td>0.337772</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700950</td>\n",
       "      <td>0.299050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900795</td>\n",
       "      <td>0.099205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.842601</td>\n",
       "      <td>0.157399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.658437</td>\n",
       "      <td>0.341563</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807960</td>\n",
       "      <td>0.192040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142650</td>\n",
       "      <td>0.857351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195835</td>\n",
       "      <td>0.804165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812928</td>\n",
       "      <td>0.187072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.901818</td>\n",
       "      <td>0.098182</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900391</td>\n",
       "      <td>0.099609</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904129</td>\n",
       "      <td>0.095871</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659234</td>\n",
       "      <td>0.340766</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.138527</td>\n",
       "      <td>0.861473</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096668</td>\n",
       "      <td>0.903332</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360153</td>\n",
       "      <td>0.639847</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900526</td>\n",
       "      <td>0.099474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902864</td>\n",
       "      <td>0.097136</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.812200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434608</td>\n",
       "      <td>0.565392</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899358</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900689</td>\n",
       "      <td>0.099311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.217398</td>\n",
       "      <td>0.782602</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.334384</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.439144</td>\n",
       "      <td>0.560856</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787865</td>\n",
       "      <td>0.212135</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.904368</td>\n",
       "      <td>0.095632</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440989</td>\n",
       "      <td>0.559011</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.172451</td>\n",
       "      <td>0.827549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062249</td>\n",
       "      <td>0.937751</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900620</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.443781</td>\n",
       "      <td>0.556219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864224</td>\n",
       "      <td>0.135776</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674386</td>\n",
       "      <td>0.325614</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.122178</td>\n",
       "      <td>0.877822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907584</td>\n",
       "      <td>0.092416</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900352</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175967</td>\n",
       "      <td>0.824033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.101650</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027363</td>\n",
       "      <td>0.972637</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886496</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.198258</td>\n",
       "      <td>0.801742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.817915</td>\n",
       "      <td>0.182085</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22494</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.244903</td>\n",
       "      <td>0.755097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907760</td>\n",
       "      <td>0.092240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>0.968880</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.866690</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142849</td>\n",
       "      <td>0.857150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.895627</td>\n",
       "      <td>0.104373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.862985</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734789</td>\n",
       "      <td>0.265211</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446445</td>\n",
       "      <td>0.553555</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.215764</td>\n",
       "      <td>0.784236</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.181782</td>\n",
       "      <td>0.818218</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132086</td>\n",
       "      <td>0.867914</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813196</td>\n",
       "      <td>0.186804</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814602</td>\n",
       "      <td>0.185397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.895149</td>\n",
       "      <td>0.104851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320747</td>\n",
       "      <td>0.679253</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367634</td>\n",
       "      <td>0.632366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.892113</td>\n",
       "      <td>0.107887</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036853</td>\n",
       "      <td>0.963147</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.443851</td>\n",
       "      <td>0.556149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804637</td>\n",
       "      <td>0.195363</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430325</td>\n",
       "      <td>0.569675</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900622</td>\n",
       "      <td>0.099378</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536303</td>\n",
       "      <td>0.463697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883753</td>\n",
       "      <td>0.116247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372204</td>\n",
       "      <td>0.627796</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.124725</td>\n",
       "      <td>0.875274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465923</td>\n",
       "      <td>0.534077</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886054</td>\n",
       "      <td>0.113946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902085</td>\n",
       "      <td>0.097915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22524</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.541312</td>\n",
       "      <td>0.458688</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22525</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.382570</td>\n",
       "      <td>0.617430</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900699</td>\n",
       "      <td>0.099301</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22527</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907958</td>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576864</td>\n",
       "      <td>0.423136</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674142</td>\n",
       "      <td>0.325858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.145954</td>\n",
       "      <td>0.854046</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.112579</td>\n",
       "      <td>0.887421</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900754</td>\n",
       "      <td>0.099246</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900951</td>\n",
       "      <td>0.099049</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22534</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.134386</td>\n",
       "      <td>0.865614</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22535</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907982</td>\n",
       "      <td>0.092018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22536</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142636</td>\n",
       "      <td>0.857364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894849</td>\n",
       "      <td>0.105151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536192</td>\n",
       "      <td>0.463808</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.890955</td>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897762</td>\n",
       "      <td>0.102238</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.884010</td>\n",
       "      <td>0.115990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.848855</td>\n",
       "      <td>0.151145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22543</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.058024</td>\n",
       "      <td>0.941976</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22544 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.133683     0.866317         1.0\n",
       "1         1.0     0.139095     0.860905         1.0\n",
       "2         0.0     0.652066     0.347934         0.0\n",
       "3         1.0     0.374286     0.625714         1.0\n",
       "4         1.0     0.316657     0.683343         1.0\n",
       "5         0.0     0.900690     0.099310         0.0\n",
       "6         0.0     0.662228     0.337772         0.0\n",
       "7         1.0     0.700950     0.299050         0.0\n",
       "8         0.0     0.900795     0.099205         0.0\n",
       "9         1.0     0.842601     0.157399         0.0\n",
       "10        1.0     0.658437     0.341563         0.0\n",
       "11        0.0     0.807960     0.192040         0.0\n",
       "12        1.0     0.142650     0.857351         1.0\n",
       "13        1.0     0.195835     0.804165         1.0\n",
       "14        0.0     0.812928     0.187072         0.0\n",
       "15        0.0     0.901818     0.098182         0.0\n",
       "16        0.0     0.900391     0.099609         0.0\n",
       "17        0.0     0.904129     0.095871         0.0\n",
       "18        0.0     0.659234     0.340766         0.0\n",
       "19        1.0     0.138527     0.861473         1.0\n",
       "20        1.0     0.096668     0.903332         1.0\n",
       "21        1.0     0.360153     0.639847         1.0\n",
       "22        0.0     0.900526     0.099474         0.0\n",
       "23        0.0     0.902864     0.097136         0.0\n",
       "24        1.0     0.187800     0.812200         1.0\n",
       "25        1.0     0.434608     0.565392         1.0\n",
       "26        0.0     0.899358     0.100642         0.0\n",
       "27        0.0     0.900689     0.099311         0.0\n",
       "28        1.0     0.217398     0.782602         1.0\n",
       "29        0.0     0.665616     0.334384         0.0\n",
       "30        1.0     0.439144     0.560856         1.0\n",
       "31        0.0     0.787865     0.212135         0.0\n",
       "32        0.0     0.904368     0.095632         0.0\n",
       "33        0.0     0.440989     0.559011         1.0\n",
       "34        1.0     0.172451     0.827549         1.0\n",
       "35        1.0     0.062249     0.937751         1.0\n",
       "36        0.0     0.900620     0.099380         0.0\n",
       "37        1.0     0.443781     0.556219         1.0\n",
       "38        0.0     0.864224     0.135776         0.0\n",
       "39        0.0     0.674386     0.325614         0.0\n",
       "40        1.0     0.122178     0.877822         1.0\n",
       "41        0.0     0.907584     0.092416         0.0\n",
       "42        0.0     0.900352     0.099648         0.0\n",
       "43        0.0     0.899083     0.100917         0.0\n",
       "44        1.0     0.175967     0.824033         1.0\n",
       "45        0.0     0.898350     0.101650         0.0\n",
       "46        1.0     0.027363     0.972637         1.0\n",
       "47        1.0     0.886496     0.113504         0.0\n",
       "48        1.0     0.198258     0.801742         1.0\n",
       "49        0.0     0.817915     0.182085         0.0\n",
       "...       ...          ...          ...         ...\n",
       "22494     1.0     0.244903     0.755097         1.0\n",
       "22495     0.0     0.907760     0.092240         0.0\n",
       "22496     1.0     0.031120     0.968880         1.0\n",
       "22497     1.0     0.133310     0.866690         1.0\n",
       "22498     1.0     0.142849     0.857150         1.0\n",
       "22499     0.0     0.895627     0.104373         0.0\n",
       "22500     1.0     0.137015     0.862985         1.0\n",
       "22501     1.0     0.734789     0.265211         0.0\n",
       "22502     1.0     0.446445     0.553555         1.0\n",
       "22503     1.0     0.215764     0.784236         1.0\n",
       "22504     1.0     0.181782     0.818218         1.0\n",
       "22505     1.0     0.132086     0.867914         1.0\n",
       "22506     0.0     0.813196     0.186804         0.0\n",
       "22507     0.0     0.814602     0.185397         0.0\n",
       "22508     0.0     0.895149     0.104851         0.0\n",
       "22509     1.0     0.320747     0.679253         1.0\n",
       "22510     1.0     0.367634     0.632366         1.0\n",
       "22511     0.0     0.892113     0.107887         0.0\n",
       "22512     1.0     0.036853     0.963147         1.0\n",
       "22513     1.0     0.443851     0.556149         1.0\n",
       "22514     0.0     0.804637     0.195363         0.0\n",
       "22515     1.0     0.430325     0.569675         1.0\n",
       "22516     0.0     0.900622     0.099378         0.0\n",
       "22517     1.0     0.536303     0.463697         0.0\n",
       "22518     0.0     0.883753     0.116247         0.0\n",
       "22519     1.0     0.372204     0.627796         1.0\n",
       "22520     1.0     0.124725     0.875274         1.0\n",
       "22521     1.0     0.465923     0.534077         1.0\n",
       "22522     1.0     0.886054     0.113946         0.0\n",
       "22523     0.0     0.902085     0.097915         0.0\n",
       "22524     1.0     0.541312     0.458688         0.0\n",
       "22525     1.0     0.382570     0.617430         1.0\n",
       "22526     0.0     0.900699     0.099301         0.0\n",
       "22527     0.0     0.907958     0.092042         0.0\n",
       "22528     1.0     0.576864     0.423136         0.0\n",
       "22529     0.0     0.674142     0.325858         0.0\n",
       "22530     1.0     0.145954     0.854046         1.0\n",
       "22531     1.0     0.112579     0.887421         1.0\n",
       "22532     0.0     0.900754     0.099246         0.0\n",
       "22533     0.0     0.900951     0.099049         0.0\n",
       "22534     1.0     0.134386     0.865614         1.0\n",
       "22535     0.0     0.907982     0.092018         0.0\n",
       "22536     1.0     0.142636     0.857364         1.0\n",
       "22537     1.0     0.894849     0.105151         0.0\n",
       "22538     1.0     0.536192     0.463808         0.0\n",
       "22539     0.0     0.890955     0.109045         0.0\n",
       "22540     0.0     0.897762     0.102238         0.0\n",
       "22541     1.0     0.884010     0.115990         0.0\n",
       "22542     0.0     0.848855     0.151145         0.0\n",
       "22543     1.0     0.058024     0.941976         1.0\n",
       "\n",
       "[22544 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions[\"10_30_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Attack_prob</th>\n",
       "      <th>Normal_prob</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.860592</td>\n",
       "      <td>0.139408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.704482</td>\n",
       "      <td>0.295518</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.177290</td>\n",
       "      <td>0.822710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.663254</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527793</td>\n",
       "      <td>0.472207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854699</td>\n",
       "      <td>0.145301</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.102281</td>\n",
       "      <td>0.897719</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.415758</td>\n",
       "      <td>0.584242</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705942</td>\n",
       "      <td>0.294058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569608</td>\n",
       "      <td>0.430392</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.789694</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.208534</td>\n",
       "      <td>0.791466</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.292663</td>\n",
       "      <td>0.707337</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.820573</td>\n",
       "      <td>0.179427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.244047</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093963</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.403173</td>\n",
       "      <td>0.596827</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.216433</td>\n",
       "      <td>0.783567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.251745</td>\n",
       "      <td>0.748255</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.469020</td>\n",
       "      <td>0.530980</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253462</td>\n",
       "      <td>0.746538</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.668657</td>\n",
       "      <td>0.331343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697148</td>\n",
       "      <td>0.302852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741392</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422326</td>\n",
       "      <td>0.577674</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.819706</td>\n",
       "      <td>0.180294</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885908</td>\n",
       "      <td>0.114092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.668456</td>\n",
       "      <td>0.331544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840838</td>\n",
       "      <td>0.159162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.089986</td>\n",
       "      <td>0.910014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396730</td>\n",
       "      <td>0.603270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.730024</td>\n",
       "      <td>0.269976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.856978</td>\n",
       "      <td>0.143022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791110</td>\n",
       "      <td>0.208890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268359</td>\n",
       "      <td>0.731641</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507439</td>\n",
       "      <td>0.492561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.129291</td>\n",
       "      <td>0.870709</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433227</td>\n",
       "      <td>0.566773</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109706</td>\n",
       "      <td>0.890295</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.851052</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900992</td>\n",
       "      <td>0.099008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875263</td>\n",
       "      <td>0.124737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258557</td>\n",
       "      <td>0.741443</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.794297</td>\n",
       "      <td>0.205703</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.916005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117057</td>\n",
       "      <td>0.882943</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125080</td>\n",
       "      <td>0.874920</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.820324</td>\n",
       "      <td>0.179676</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.146411</td>\n",
       "      <td>0.853589</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.203138</td>\n",
       "      <td>0.796862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11800</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596107</td>\n",
       "      <td>0.403893</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271017</td>\n",
       "      <td>0.728983</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736911</td>\n",
       "      <td>0.263089</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.742166</td>\n",
       "      <td>0.257834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735452</td>\n",
       "      <td>0.264548</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11805</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.410258</td>\n",
       "      <td>0.589742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.855157</td>\n",
       "      <td>0.144843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11807</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.747397</td>\n",
       "      <td>0.252603</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>0.935631</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.247243</td>\n",
       "      <td>0.752757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659739</td>\n",
       "      <td>0.340261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436106</td>\n",
       "      <td>0.563894</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764235</td>\n",
       "      <td>0.235765</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11813</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236302</td>\n",
       "      <td>0.763698</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11815</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671653</td>\n",
       "      <td>0.328347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11816</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740753</td>\n",
       "      <td>0.259247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847111</td>\n",
       "      <td>0.152889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840581</td>\n",
       "      <td>0.159419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057207</td>\n",
       "      <td>0.942793</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821950</td>\n",
       "      <td>0.178050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474174</td>\n",
       "      <td>0.525826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185466</td>\n",
       "      <td>0.814534</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076480</td>\n",
       "      <td>0.923520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832676</td>\n",
       "      <td>0.167324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864765</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.131697</td>\n",
       "      <td>0.868303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511987</td>\n",
       "      <td>0.488013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731790</td>\n",
       "      <td>0.268210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.207584</td>\n",
       "      <td>0.792416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.641705</td>\n",
       "      <td>0.358295</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373179</td>\n",
       "      <td>0.626821</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.147031</td>\n",
       "      <td>0.852969</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571376</td>\n",
       "      <td>0.428624</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372231</td>\n",
       "      <td>0.627769</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.855495</td>\n",
       "      <td>0.144505</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.407721</td>\n",
       "      <td>0.592279</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.192555</td>\n",
       "      <td>0.807445</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.492263</td>\n",
       "      <td>0.507737</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036314</td>\n",
       "      <td>0.963686</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492791</td>\n",
       "      <td>0.507209</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740603</td>\n",
       "      <td>0.259397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.403213</td>\n",
       "      <td>0.596787</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224630</td>\n",
       "      <td>0.775371</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.732751</td>\n",
       "      <td>0.267249</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.846432</td>\n",
       "      <td>0.153568</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.891339</td>\n",
       "      <td>0.108661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269681</td>\n",
       "      <td>0.730319</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057944</td>\n",
       "      <td>0.942056</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821950</td>\n",
       "      <td>0.178050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11850 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Actual  Attack_prob  Normal_prob  Prediction\n",
       "0         1.0     0.860592     0.139408         0.0\n",
       "1         1.0     0.704482     0.295518         0.0\n",
       "2         1.0     0.177290     0.822710         1.0\n",
       "3         0.0     0.663254     0.336746         0.0\n",
       "4         1.0     0.527793     0.472207         0.0\n",
       "5         1.0     0.854699     0.145301         0.0\n",
       "6         1.0     0.102281     0.897719         1.0\n",
       "7         1.0     0.415758     0.584242         1.0\n",
       "8         0.0     0.705942     0.294058         0.0\n",
       "9         0.0     0.569608     0.430392         0.0\n",
       "10        1.0     0.210306     0.789694         1.0\n",
       "11        1.0     0.208534     0.791466         1.0\n",
       "12        1.0     0.292663     0.707337         1.0\n",
       "13        1.0     0.820573     0.179427         0.0\n",
       "14        1.0     0.244047     0.755952         1.0\n",
       "15        1.0     0.093963     0.906037         1.0\n",
       "16        1.0     0.403173     0.596827         1.0\n",
       "17        1.0     0.216433     0.783567         1.0\n",
       "18        1.0     0.251745     0.748255         1.0\n",
       "19        1.0     0.469020     0.530980         1.0\n",
       "20        1.0     0.253462     0.746538         1.0\n",
       "21        1.0     0.668657     0.331343         0.0\n",
       "22        1.0     0.697148     0.302852         0.0\n",
       "23        1.0     0.741392     0.258608         0.0\n",
       "24        1.0     0.422326     0.577674         1.0\n",
       "25        1.0     0.819706     0.180294         0.0\n",
       "26        1.0     0.885908     0.114092         0.0\n",
       "27        1.0     0.668456     0.331544         0.0\n",
       "28        0.0     0.840838     0.159162         0.0\n",
       "29        1.0     0.089986     0.910014         1.0\n",
       "30        1.0     0.396730     0.603270         1.0\n",
       "31        1.0     0.730024     0.269976         0.0\n",
       "32        0.0     0.856978     0.143022         0.0\n",
       "33        0.0     0.791110     0.208890         0.0\n",
       "34        1.0     0.268359     0.731641         1.0\n",
       "35        0.0     0.507439     0.492561         0.0\n",
       "36        1.0     0.129291     0.870709         1.0\n",
       "37        1.0     0.433227     0.566773         1.0\n",
       "38        1.0     0.109706     0.890295         1.0\n",
       "39        1.0     0.148948     0.851052         1.0\n",
       "40        0.0     0.900992     0.099008         0.0\n",
       "41        1.0     0.875263     0.124737         0.0\n",
       "42        1.0     0.258557     0.741443         1.0\n",
       "43        0.0     0.794297     0.205703         0.0\n",
       "44        1.0     0.083995     0.916005         1.0\n",
       "45        1.0     0.117057     0.882943         1.0\n",
       "46        1.0     0.125080     0.874920         1.0\n",
       "47        1.0     0.820324     0.179676         0.0\n",
       "48        1.0     0.146411     0.853589         1.0\n",
       "49        1.0     0.203138     0.796862         1.0\n",
       "...       ...          ...          ...         ...\n",
       "11800     1.0     0.596107     0.403893         0.0\n",
       "11801     1.0     0.271017     0.728983         1.0\n",
       "11802     0.0     0.736911     0.263089         0.0\n",
       "11803     1.0     0.742166     0.257834         0.0\n",
       "11804     1.0     0.735452     0.264548         0.0\n",
       "11805     1.0     0.410258     0.589742         1.0\n",
       "11806     0.0     0.855157     0.144843         0.0\n",
       "11807     1.0     0.747397     0.252603         0.0\n",
       "11808     1.0     0.064368     0.935631         1.0\n",
       "11809     1.0     0.247243     0.752757         1.0\n",
       "11810     1.0     0.659739     0.340261         0.0\n",
       "11811     1.0     0.436106     0.563894         1.0\n",
       "11812     0.0     0.764235     0.235765         0.0\n",
       "11813     1.0     0.236302     0.763698         1.0\n",
       "11814     1.0     0.000441     0.999559         1.0\n",
       "11815     1.0     0.671653     0.328347         0.0\n",
       "11816     1.0     0.740753     0.259247         0.0\n",
       "11817     0.0     0.847111     0.152889         0.0\n",
       "11818     0.0     0.840581     0.159419         0.0\n",
       "11819     1.0     0.057207     0.942793         1.0\n",
       "11820     1.0     0.821950     0.178050         0.0\n",
       "11821     1.0     0.474174     0.525826         1.0\n",
       "11822     1.0     0.185466     0.814534         1.0\n",
       "11823     1.0     0.076480     0.923520         1.0\n",
       "11824     1.0     0.832676     0.167324         0.0\n",
       "11825     0.0     0.864765     0.135235         0.0\n",
       "11826     1.0     0.131697     0.868303         1.0\n",
       "11827     1.0     0.511987     0.488013         0.0\n",
       "11828     1.0     0.731790     0.268210         0.0\n",
       "11829     1.0     0.207584     0.792416         1.0\n",
       "11830     1.0     0.641705     0.358295         0.0\n",
       "11831     1.0     0.373179     0.626821         1.0\n",
       "11832     1.0     0.147031     0.852969         1.0\n",
       "11833     0.0     0.571376     0.428624         0.0\n",
       "11834     1.0     0.372231     0.627769         1.0\n",
       "11835     0.0     0.855495     0.144505         0.0\n",
       "11836     1.0     0.407721     0.592279         1.0\n",
       "11837     1.0     0.192555     0.807445         1.0\n",
       "11838     1.0     0.492263     0.507737         1.0\n",
       "11839     1.0     0.036314     0.963686         1.0\n",
       "11840     0.0     0.492791     0.507209         1.0\n",
       "11841     0.0     0.740603     0.259397         0.0\n",
       "11842     1.0     0.403213     0.596787         1.0\n",
       "11843     1.0     0.224630     0.775371         1.0\n",
       "11844     1.0     0.732751     0.267249         0.0\n",
       "11845     0.0     0.846432     0.153568         0.0\n",
       "11846     0.0     0.891339     0.108661         0.0\n",
       "11847     1.0     0.269681     0.730319         1.0\n",
       "11848     1.0     0.057944     0.942056         1.0\n",
       "11849     1.0     0.821950     0.178050         0.0\n",
       "\n",
       "[11850 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.predictions_[\"10_30_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Train.predictions[\"10_30_3\"].dropna()\n",
    "df_ = Train.predictions_[\"10_30_3\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as me\n",
    "def get_score(y_true, y_pred):\n",
    "    f1 = me.f1_score(y_true, y_pred)\n",
    "    pre = me.precision_score(y_true, y_pred)\n",
    "    rec = me.recall_score(y_true, y_pred)\n",
    "    acc = me.accuracy_score(y_true, y_pred)\n",
    "    return {\"F1 Score\":f1, \"Precision\":pre, \"Recall\":rec, \"Accuracy\":acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.810060</td>\n",
       "      <td>0.806385</td>\n",
       "      <td>0.960573</td>\n",
       "      <td>0.694849</td>\n",
       "      <td>Train+/Test+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.642278</td>\n",
       "      <td>0.732369</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.598061</td>\n",
       "      <td>Train+/Test-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score  Precision    Recall      Scenario\n",
       "0  0.810060  0.806385   0.960573  0.694849  Train+/Test+\n",
       "1  0.642278  0.732369   0.944472  0.598061  Train+/Test-"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "\n",
    "scores = get_score(df.loc[:,'Actual'].values.astype(int),\n",
    "                df.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test+\"})\n",
    "score_df = pd.DataFrame(scores, index=[0])\n",
    "\n",
    "scores = get_score(df_.loc[:,'Actual'].values.astype(int),\n",
    "                df_.loc[:,'Prediction'].values.astype(int))\n",
    "scores.update({\"Scenario\":\"Train+/Test-\"})\n",
    "\n",
    "score_df = score_df.append(pd.DataFrame(scores, index=[1]))\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     9711\n",
       "1.0    12833\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xefca208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0    2152\n",
       "1.0    9698\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27e82c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df_.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df_.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
